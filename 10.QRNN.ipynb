{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.QRNN - Quasi-Recurrent Neural Networks\n",
    "In this paper, authors introduce a RNN-like architecture which is faster than RNNs. This fast implementation is possible with the use of convolution layer which supports parallel computations not possible in RNN. Let's see the performance of this faster architecture!\n",
    "\n",
    "**Comment**\n",
    "\n",
    "I failed to efficiently implement QRNN. This model works, but it gets very slower when we expand the layer's timestep. This is maybe due to the GPU memory problem. So if you run this model on a big dataset like IMDB, it may not work properly. Anyway, let's see how this model works. Even though it does not work efficiently in this implementation, it's working for small datasets like SST, PTB.\n",
    "\n",
    "### References\n",
    "- [Quasi-Recurrent Neural Networks - Bradbury et al. 2016](https://arxiv.org/abs/1611.01576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Preprocessing codes and processed data are borrowed from [harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch).\n",
    "\n",
    "It's getting harder and harder to preprecess data in our model class. So we will preprocess before using `fit_to_corpus()` method as far as we can.\n",
    "\n",
    "You have to select among these datasets `MR/SST1/SST2/Subj/TREC/CR/MPQA/IMDB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.sentiment_datasets.preprocess as preprocess\n",
    "from models import QRNN\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST2.pkl exists! loading from pkl..\n"
     ]
    }
   ],
   "source": [
    "w2v, train, train_label, test, test_label, dev, dev_label, word_to_idx = preprocess.build_dataset(\"SST2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev, train_label, test_label, dev_label = \\\n",
    "    preprocess.train_test_dev_split(train, test, dev, train_label, test_label, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [train, train_label, dev, dev_label, w2v, word_to_idx]\n",
    "test_data = [test, test_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - SST2\n",
    "If you perform some more hyperparameter tuning, you might be able to get a better result than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04231523\n"
     ]
    }
   ],
   "source": [
    "model = QRNN.QRNN(batch_size = 24,\n",
    "                  dropout_keep_prob = 0.7,\n",
    "                  zoneout_keep_prob = 1.0,\n",
    "                  learning_rate = 0.0005,\n",
    "                  filter_windows = [2,2],\n",
    "                  l2_reg_lambda = 4e-6,\n",
    "                  hidden_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_to_corpus(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 5711230\n",
      "--------------------------------------------------------------------------------\n",
      "000100: 1 [00100/00288], train_loss = 0.70546216, accuracy = 0.58333331, secs/batch = 0.0123\n",
      "000200: 1 [00200/00288], train_loss = 0.71992451, accuracy = 0.54166669, secs/batch = 0.0129\n",
      "Epoch training time: 5.223785638809204\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 0.66215535, train_accruacy = 0.57826968\n",
      "valid_loss = 0.57006465, valid_accuracy = 0.70370370\n",
      "\n",
      "000388: 2 [00100/00288], train_loss = 0.57274020, accuracy = 0.75000000, secs/batch = 0.0129\n",
      "000488: 2 [00200/00288], train_loss = 0.42499664, accuracy = 0.79166669, secs/batch = 0.0131\n",
      "Epoch training time: 3.7058911323547363\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 0.57065903, train_accruacy = 0.70095486\n",
      "valid_loss = 0.53854320, valid_accuracy = 0.73379630\n",
      "\n",
      "000676: 3 [00100/00288], train_loss = 0.45365071, accuracy = 0.83333331, secs/batch = 0.0131\n",
      "000776: 3 [00200/00288], train_loss = 0.67073113, accuracy = 0.58333331, secs/batch = 0.0125\n",
      "Epoch training time: 3.7044947147369385\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 0.52628657, train_accruacy = 0.73480903\n",
      "valid_loss = 0.50585898, valid_accuracy = 0.73032407\n",
      "\n",
      "000964: 4 [00100/00288], train_loss = 0.29463002, accuracy = 0.87500000, secs/batch = 0.0130\n",
      "001064: 4 [00200/00288], train_loss = 0.34399408, accuracy = 0.87500000, secs/batch = 0.0123\n",
      "Epoch training time: 3.718135118484497\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 0.46009765, train_accruacy = 0.77618634\n",
      "valid_loss = 0.43463410, valid_accuracy = 0.78819445\n",
      "\n",
      "001252: 5 [00100/00288], train_loss = 0.59842539, accuracy = 0.70833331, secs/batch = 0.0137\n",
      "001352: 5 [00200/00288], train_loss = 0.30605102, accuracy = 0.91666669, secs/batch = 0.0130\n",
      "Epoch training time: 3.754948616027832\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 0.40030231, train_accruacy = 0.81698495\n",
      "valid_loss = 0.39063312, valid_accuracy = 0.82986111\n",
      "\n",
      "001540: 6 [00100/00288], train_loss = 0.28329071, accuracy = 0.87500000, secs/batch = 0.0126\n",
      "001640: 6 [00200/00288], train_loss = 0.45988914, accuracy = 0.70833331, secs/batch = 0.0126\n",
      "Epoch training time: 3.709245204925537\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 0.36372800, train_accruacy = 0.83781829\n",
      "valid_loss = 0.40276766, valid_accuracy = 0.81944444\n",
      "\n",
      "001828: 7 [00100/00288], train_loss = 0.23075327, accuracy = 0.95833331, secs/batch = 0.0130\n",
      "001928: 7 [00200/00288], train_loss = 0.41941527, accuracy = 0.79166669, secs/batch = 0.0125\n",
      "Epoch training time: 3.7190563678741455\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 0.34133742, train_accruacy = 0.85083912\n",
      "valid_loss = 0.38565168, valid_accuracy = 0.83217593\n",
      "\n",
      "002116: 8 [00100/00288], train_loss = 0.36292544, accuracy = 0.83333331, secs/batch = 0.0131\n",
      "002216: 8 [00200/00288], train_loss = 0.20129140, accuracy = 0.95833331, secs/batch = 0.0129\n",
      "Epoch training time: 3.730466365814209\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 0.30594834, train_accruacy = 0.87051505\n",
      "valid_loss = 0.39749245, valid_accuracy = 0.82175926\n",
      "\n",
      "002404: 9 [00100/00288], train_loss = 0.14779593, accuracy = 1.00000000, secs/batch = 0.0134\n",
      "002504: 9 [00200/00288], train_loss = 0.15012242, accuracy = 0.95833331, secs/batch = 0.0135\n",
      "Epoch training time: 3.775581121444702\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 0.28931564, train_accruacy = 0.87745949\n",
      "valid_loss = 0.38009098, valid_accuracy = 0.83449074\n",
      "\n",
      "002692: 10 [00100/00288], train_loss = 0.32014850, accuracy = 0.87500000, secs/batch = 0.0128\n",
      "002792: 10 [00200/00288], train_loss = 0.30635536, accuracy = 0.91666669, secs/batch = 0.0128\n",
      "Epoch training time: 3.724318265914917\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 0.26106882, train_accruacy = 0.89221644\n",
      "valid_loss = 0.38612238, valid_accuracy = 0.84027777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(10, save_dir=\"save/10_qrnn/sst2\", log_dir=\"log/10_qrnn/sst2\", print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/10_qrnn/sst2/epoch010_0.3861.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 5711230\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 0.33483320, test accuracy = 0.85777778\n",
      "test samples: 001800, time elapsed: 0.5791, time per one batch: 0.0077\n"
     ]
    }
   ],
   "source": [
    "model.test(test_data, load_dir=\"save/10_qrnn/sst2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - PTB\n",
    "Let's see how this model works for Language Model. Even though i have managed to get 83.88 perplexity which is higher than the paper's result, I'm sure you'll be able to reproduce paper's result if you do some more hyperparameter tunings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.rnnlm_datasets.preprocess as preprocess\n",
    "from models import QRNN_LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "actual longest token length is: 21\n",
      "size of word vocabulary: 10000\n",
      "size of char vocabulary: 51\n",
      "number of tokens in train: 929589\n",
      "number of tokens in valid: 73760\n",
      "number of tokens in test: 82430\n"
     ]
    }
   ],
   "source": [
    "word_to_idx, char_to_idx, word_tensors, char_tensors, actual_max_word_length = \\\n",
    "    preprocess.build_dataset(\"ptb\", 30, eos='+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word, valid_word, test_word, train_char, valid_char, test_char = \\\n",
    "    preprocess.train_test_dev_split(word_tensors, char_tensors)\n",
    "\n",
    "train_data = [train_word, valid_word, word_to_idx]\n",
    "test_data = [test_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04231523\n"
     ]
    }
   ],
   "source": [
    "model = QRNN_LM.QRNN_LM(batch_size = 20,\n",
    "                        dropout_keep_prob = 0.5,\n",
    "                        learning_rate = 1.0,\n",
    "                        filter_windows = [2,2],\n",
    "                        l2_reg_lambda = 2e-3,\n",
    "                        hidden_size = 640,\n",
    "                        num_unroll_steps = 105,\n",
    "                        grad_clip = 10.0,\n",
    "                        zoneout_keep_prob = 0.9,\n",
    "                        word_embedding_size = 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_corpus(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(72, save_dir=\"save/10_qrnn/ptb\", log_dir=\"log/10_qrnn/ptb\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/10_qrnn/ptb/epoch072_4.4826.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 17726481\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 4.42936604, perplexity = 83.87822448\n",
      "test samples: 000780, time elapsed: 1.4223, time per one batch: 0.0365\n"
     ]
    }
   ],
   "source": [
    "model.test(test_data, load_dir=\"save/10_qrnn/ptb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
