{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. WordRNN - RNN Language Model with Words\n",
    "\n",
    "We will construct a Language Model with Recurrent Neural Networks. In this notebook, we will only cover one-way RNN/GRU/LSTM language model but it will be possible to expand one way network with one layer to bi-directional deep recurrent neural networks. Also, it is also possible to use CNN with RNN in order to construct a language model. We will cover it in a near future.\n",
    "\n",
    "Also, We will do some funny sentence generating from tinyshakespeare dataset. Although the primary goal of WordRNN.py is to build a language model and train/test on a PTB dataset, I've added some methods for sentence generating. But it's not optimized for sentence generating since `train()` method requires valid set for evaluating currently, which is not needed for sampling. I'm planning to fix it but it's not in my priority.\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 8](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 9](http://web.stanford.edu/class/cs224n/lectures/lecture9.pdf)\n",
    "- [mkroutikov/tf-lstm-char-cnn](https://github.com/mkroutikov/tf-lstm-char-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WordRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(data_dir):\n",
    "    corpus = []\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            tmp_line = line.strip().split(' ')\n",
    "            if len(tmp_line) == 1:\n",
    "                continue\n",
    "            corpus.append(tmp_line)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Penn Tree Bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = read_corpus(\"data/rnnlm_datasets/ptb/train.txt\")\n",
    "valid_corpus = read_corpus(\"data/rnnlm_datasets/ptb/valid.txt\")\n",
    "test_corpus = read_corpus(\"data/rnnlm_datasets/ptb/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you modify some parts of RNN_LM.py, you will be able to use pretrained word embeddings for this model and compare the performance of the cases when you use pretrained embeddings or not. I'll use pretrained embedding in future models, but in this model i'll leave it to readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04152210\n"
     ]
    }
   ],
   "source": [
    "model = WordRNN.WordRNN(word_embedding_size=128,\n",
    "                        hidden_size=512,\n",
    "                        cell=\"LSTM\",\n",
    "                        num_unroll_steps=30,\n",
    "                        learning_rate=0.001,\n",
    "                        batch_size=64,\n",
    "                        num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_corpus(train_corpus, valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 9821327\n",
      "--------------------------------------------------------------------------------\n",
      "000200: 1 [00200/00484], train_loss/perplexity = 6.64135027/766.1287842 secs/batch = 0.0417\n",
      "000400: 1 [00400/00484], train_loss/perplexity = 6.61470509/745.9846802 secs/batch = 0.0426\n",
      "Epoch training time: 20.554283380508423\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 6.75112559, perplexity = 855.02062786\n",
      "validation_loss = 6.66651878, perplexity = 785.65579451\n",
      "\n",
      "000684: 2 [00200/00484], train_loss/perplexity = 6.55243683/700.9501953 secs/batch = 0.0427\n",
      "000884: 2 [00400/00484], train_loss/perplexity = 6.29491997/541.8125000 secs/batch = 0.0428\n",
      "Epoch training time: 20.479009866714478\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 6.53963027, perplexity = 692.03066569\n",
      "validation_loss = 6.29975884, perplexity = 544.44059474\n",
      "\n",
      "001168: 3 [00200/00484], train_loss/perplexity = 5.96246672/388.5674438 secs/batch = 0.0426\n",
      "001368: 3 [00400/00484], train_loss/perplexity = 5.81537724/335.4179077 secs/batch = 0.0423\n",
      "Epoch training time: 20.544854640960693\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 6.00385299, perplexity = 404.98620062\n",
      "validation_loss = 5.87936507, perplexity = 357.58212918\n",
      "\n",
      "001652: 4 [00200/00484], train_loss/perplexity = 5.52704620/251.4002228 secs/batch = 0.0422\n",
      "001852: 4 [00400/00484], train_loss/perplexity = 5.52877522/251.8352814 secs/batch = 0.0426\n",
      "Epoch training time: 20.564638137817383\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 5.62317739, perplexity = 276.76738594\n",
      "validation_loss = 5.48766158, perplexity = 241.69136840\n",
      "\n",
      "002136: 5 [00200/00484], train_loss/perplexity = 5.29250383/198.8406677 secs/batch = 0.0426\n",
      "002336: 5 [00400/00484], train_loss/perplexity = 5.37564421/216.0790253 secs/batch = 0.0423\n",
      "Epoch training time: 20.590905904769897\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 5.42580521, perplexity = 227.19421198\n",
      "validation_loss = 5.36711670, perplexity = 214.24424634\n",
      "\n",
      "002620: 6 [00200/00484], train_loss/perplexity = 5.14547539/171.6530609 secs/batch = 0.0434\n",
      "002820: 6 [00400/00484], train_loss/perplexity = 5.26815128/194.0568695 secs/batch = 0.0425\n",
      "Epoch training time: 20.642998456954956\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 5.28966201, perplexity = 198.27639775\n",
      "validation_loss = 5.25964206, perplexity = 192.41260708\n",
      "\n",
      "003104: 7 [00200/00484], train_loss/perplexity = 5.01544714/150.7235107 secs/batch = 0.0429\n",
      "003304: 7 [00400/00484], train_loss/perplexity = 5.13706493/170.2154388 secs/batch = 0.0431\n",
      "Epoch training time: 20.597008228302002\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 5.18433389, perplexity = 178.45453983\n",
      "validation_loss = 5.20274368, perplexity = 181.77027797\n",
      "\n",
      "003588: 8 [00200/00484], train_loss/perplexity = 4.93487787/139.0561523 secs/batch = 0.0418\n",
      "003788: 8 [00400/00484], train_loss/perplexity = 5.06882858/158.9879761 secs/batch = 0.0432\n",
      "Epoch training time: 20.69495916366577\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 5.09510918, perplexity = 163.22166387\n",
      "validation_loss = 5.14555965, perplexity = 171.66753148\n",
      "\n",
      "004072: 9 [00200/00484], train_loss/perplexity = 4.80480671/122.0958862 secs/batch = 0.0419\n",
      "004272: 9 [00400/00484], train_loss/perplexity = 4.95981836/142.5679016 secs/batch = 0.0444\n",
      "Epoch training time: 20.689464807510376\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 4.99431168, perplexity = 147.57133433\n",
      "validation_loss = 5.11112730, perplexity = 165.85722058\n",
      "\n",
      "004556: 10 [00200/00484], train_loss/perplexity = 4.75439262/116.0931168 secs/batch = 0.0434\n",
      "004756: 10 [00400/00484], train_loss/perplexity = 4.96177101/142.8465576 secs/batch = 0.0433\n",
      "Epoch training time: 20.70923089981079\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 4.95377135, perplexity = 141.70839003\n",
      "validation_loss = 5.07481567, perplexity = 159.94270780\n",
      "\n",
      "005040: 11 [00200/00484], train_loss/perplexity = 4.72248554/112.4473953 secs/batch = 0.0425\n",
      "005240: 11 [00400/00484], train_loss/perplexity = 4.85461712/128.3315430 secs/batch = 0.0423\n",
      "Epoch training time: 20.739701747894287\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 11\n",
      "train_loss = 4.89124459, perplexity = 133.11915038\n",
      "validation_loss = 5.04234506, perplexity = 154.83268148\n",
      "\n",
      "005524: 12 [00200/00484], train_loss/perplexity = 4.61133862/100.6187515 secs/batch = 0.0422\n",
      "005724: 12 [00400/00484], train_loss/perplexity = 4.82205439/124.2200241 secs/batch = 0.0416\n",
      "Epoch training time: 20.69998025894165\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 12\n",
      "train_loss = 4.83311302, perplexity = 125.60135198\n",
      "validation_loss = 5.02429123, perplexity = 152.06244012\n",
      "\n",
      "006008: 13 [00200/00484], train_loss/perplexity = 4.57587433/97.1129074 secs/batch = 0.0424\n",
      "006208: 13 [00400/00484], train_loss/perplexity = 4.76103354/116.8666458 secs/batch = 0.0427\n",
      "Epoch training time: 20.715762615203857\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 13\n",
      "train_loss = 4.78264828, perplexity = 119.42018994\n",
      "validation_loss = 5.01888374, perplexity = 151.24238413\n",
      "\n",
      "006492: 14 [00200/00484], train_loss/perplexity = 4.53259945/92.9999924 secs/batch = 0.0427\n",
      "006692: 14 [00400/00484], train_loss/perplexity = 4.73774529/114.1764755 secs/batch = 0.0423\n",
      "Epoch training time: 20.698445081710815\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 14\n",
      "train_loss = 4.73793967, perplexity = 114.19867275\n",
      "validation_loss = 5.04160077, perplexity = 154.71748354\n",
      "\n",
      "006976: 15 [00200/00484], train_loss/perplexity = 4.49950409/89.9725037 secs/batch = 0.0428\n",
      "007176: 15 [00400/00484], train_loss/perplexity = 4.68648863/108.4716263 secs/batch = 0.0426\n",
      "Epoch training time: 20.679969549179077\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 15\n",
      "train_loss = 4.71864893, perplexity = 112.01680819\n",
      "validation_loss = 4.99894447, perplexity = 148.25658732\n",
      "\n",
      "007460: 16 [00200/00484], train_loss/perplexity = 4.45564556/86.1117249 secs/batch = 0.0423\n",
      "007660: 16 [00400/00484], train_loss/perplexity = 4.63487768/103.0153198 secs/batch = 0.0433\n",
      "Epoch training time: 20.67658543586731\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 16\n",
      "train_loss = 4.65444876, perplexity = 105.05129519\n",
      "validation_loss = 4.98555151, perplexity = 146.28422968\n",
      "\n",
      "007944: 17 [00200/00484], train_loss/perplexity = 4.42929935/83.8726349 secs/batch = 0.0435\n",
      "008144: 17 [00400/00484], train_loss/perplexity = 4.60082340/99.5662613 secs/batch = 0.0431\n",
      "Epoch training time: 20.713943004608154\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 17\n",
      "train_loss = 4.61930419, perplexity = 101.42343613\n",
      "validation_loss = 4.96169706, perplexity = 142.83599214\n",
      "\n",
      "008428: 18 [00200/00484], train_loss/perplexity = 4.38701725/80.4002457 secs/batch = 0.0429\n",
      "008628: 18 [00400/00484], train_loss/perplexity = 4.57150745/96.6897583 secs/batch = 0.0427\n",
      "Epoch training time: 20.77534055709839\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 18\n",
      "train_loss = 4.58212733, perplexity = 97.72206048\n",
      "validation_loss = 4.97099579, perplexity = 144.17037935\n",
      "\n",
      "008912: 19 [00200/00484], train_loss/perplexity = 4.36851978/78.9267197 secs/batch = 0.0429\n",
      "009112: 19 [00400/00484], train_loss/perplexity = 4.54025984/93.7151489 secs/batch = 0.0435\n",
      "Epoch training time: 20.724308013916016\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 19\n",
      "train_loss = 4.54840508, perplexity = 94.48159785\n",
      "validation_loss = 4.97290678, perplexity = 144.44615058\n",
      "\n",
      "009396: 20 [00200/00484], train_loss/perplexity = 4.28826714/72.8401337 secs/batch = 0.0423\n",
      "009596: 20 [00400/00484], train_loss/perplexity = 4.48219776/88.4288025 secs/batch = 0.0435\n",
      "Epoch training time: 20.72806668281555\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 20\n",
      "train_loss = 4.51801718, perplexity = 91.65368499\n",
      "validation_loss = 4.97144378, perplexity = 144.23498049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(20, save_dir=\"save/05_rnn_lm\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/05_rnn_lm/epoch020_4.9714.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 9821327\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 4.87975677, perplexity = 131.59865106\n",
      "test samples: 002688, time elapsed: 0.7536, time per one batch: 0.0179\n"
     ]
    }
   ],
   "source": [
    "model.test(test_corpus, load_dir=\"save/05_rnn_lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try sampling with PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/05_rnn_lm/epoch020_4.9714.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"stock will make them to be the right of the stock and exchange commission in the past century 's market in the u.s. 's history of the market 's market and that\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(30, load_dir=\"save/05_rnn_lm\", starter_word=\"stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With tinyshakespeare Data\n",
    "We will use tinyshakespeare dataset for sampling. We will compare the result from this WordRNN to CharRNN in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = read_corpus(\"data/rnn/input.txt\")\n",
    "valid_corpus = read_corpus(\"data/rnn/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04152210\n"
     ]
    }
   ],
   "source": [
    "model2 = WordRNN.WordRNN(word_embedding_size=256,\n",
    "                        hidden_size=512,\n",
    "                        cell=\"LSTM\",\n",
    "                        num_unroll_steps=30,\n",
    "                        learning_rate=0.005,\n",
    "                        batch_size=64,\n",
    "                        num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit_to_corpus(train_corpus, valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 23236703\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch training time: 8.178820133209229\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 7.85117774, perplexity = 2568.75786937\n",
      "validation_loss = 7.53823048, perplexity = 1878.50303529\n",
      "\n",
      "Epoch training time: 8.057024478912354\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 7.93559659, perplexity = 2795.02573622\n",
      "validation_loss = 7.71589308, perplexity = 2243.72582879\n",
      "\n",
      "Epoch training time: 8.102108716964722\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 8.16610162, perplexity = 3519.59647496\n",
      "validation_loss = 7.87270113, perplexity = 2624.64553354\n",
      "\n",
      "Epoch training time: 8.0701322555542\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 8.39273260, perplexity = 4414.86530375\n",
      "validation_loss = 7.97569005, perplexity = 2909.36479382\n",
      "\n",
      "Epoch training time: 8.089577913284302\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 7.24787741, perplexity = 1405.11919270\n",
      "validation_loss = 6.39514205, perplexity = 598.92839227\n",
      "\n",
      "Epoch training time: 8.083864450454712\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 6.47830064, perplexity = 650.86395197\n",
      "validation_loss = 6.09433738, perplexity = 443.34018045\n",
      "\n",
      "Epoch training time: 8.117440223693848\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 6.20150495, perplexity = 493.49115996\n",
      "validation_loss = 5.85547217, perplexity = 349.13971303\n",
      "\n",
      "Epoch training time: 8.097052097320557\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 5.99314190, perplexity = 400.67150329\n",
      "validation_loss = 5.71055924, perplexity = 302.03993284\n",
      "\n",
      "Epoch training time: 8.070464372634888\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 5.83028723, perplexity = 340.45645308\n",
      "validation_loss = 5.54128598, perplexity = 255.00572183\n",
      "\n",
      "Epoch training time: 8.108033657073975\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 5.69013242, perplexity = 295.93280687\n",
      "validation_loss = 5.41139003, perplexity = 223.94265825\n",
      "\n",
      "Epoch training time: 8.122928380966187\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 11\n",
      "train_loss = 5.56470553, perplexity = 261.04832099\n",
      "validation_loss = 5.27977833, perplexity = 196.32635056\n",
      "\n",
      "Epoch training time: 8.080579042434692\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 12\n",
      "train_loss = 5.44728453, perplexity = 232.12697546\n",
      "validation_loss = 5.17082830, perplexity = 176.06060834\n",
      "\n",
      "Epoch training time: 8.115979194641113\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 13\n",
      "train_loss = 5.34332058, perplexity = 209.20624498\n",
      "validation_loss = 5.08022407, perplexity = 160.81008396\n",
      "\n",
      "Epoch training time: 8.148609638214111\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 14\n",
      "train_loss = 5.23834630, perplexity = 188.35835596\n",
      "validation_loss = 4.96658853, perplexity = 143.53638171\n",
      "\n",
      "Epoch training time: 8.084917783737183\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 15\n",
      "train_loss = 5.13358425, perplexity = 169.62400508\n",
      "validation_loss = 4.86129489, perplexity = 129.19138198\n",
      "\n",
      "Epoch training time: 8.126396417617798\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 16\n",
      "train_loss = 5.03346927, perplexity = 153.46450065\n",
      "validation_loss = 4.74975516, perplexity = 115.55598840\n",
      "\n",
      "Epoch training time: 8.067938566207886\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 17\n",
      "train_loss = 4.93608807, perplexity = 139.22454602\n",
      "validation_loss = 4.64364917, perplexity = 103.92288889\n",
      "\n",
      "Epoch training time: 8.096905946731567\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 18\n",
      "train_loss = 4.84851236, perplexity = 127.55049964\n",
      "validation_loss = 4.53826657, perplexity = 93.52853458\n",
      "\n",
      "Epoch training time: 8.12075138092041\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 19\n",
      "train_loss = 4.75957038, perplexity = 116.69578037\n",
      "validation_loss = 4.46735101, perplexity = 87.12562244\n",
      "\n",
      "Epoch training time: 8.091320514678955\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 20\n",
      "train_loss = 4.66900472, perplexity = 106.59160068\n",
      "validation_loss = 4.39874221, perplexity = 81.34848458\n",
      "\n",
      "Epoch training time: 8.113580703735352\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 21\n",
      "train_loss = 4.57701207, perplexity = 97.22346331\n",
      "validation_loss = 4.29491449, perplexity = 73.32594409\n",
      "\n",
      "Epoch training time: 8.159294605255127\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 22\n",
      "train_loss = 4.48901118, perplexity = 89.03336407\n",
      "validation_loss = 4.17430108, perplexity = 64.99439774\n",
      "\n",
      "Epoch training time: 8.089267253875732\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 23\n",
      "train_loss = 4.39866200, perplexity = 81.34195993\n",
      "validation_loss = 4.06867449, perplexity = 58.47939619\n",
      "\n",
      "Epoch training time: 8.103075742721558\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 24\n",
      "train_loss = 4.30701073, perplexity = 74.21829839\n",
      "validation_loss = 3.97924853, perplexity = 53.47683288\n",
      "\n",
      "Epoch training time: 8.102243900299072\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 25\n",
      "train_loss = 4.22363477, perplexity = 68.28122054\n",
      "validation_loss = 3.92335335, perplexity = 50.56973898\n",
      "\n",
      "Epoch training time: 8.125754594802856\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 26\n",
      "train_loss = 4.13559336, perplexity = 62.52668069\n",
      "validation_loss = 3.89469572, perplexity = 49.14109862\n",
      "\n",
      "Epoch training time: 8.124781847000122\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 27\n",
      "train_loss = 4.05668956, perplexity = 57.78270771\n",
      "validation_loss = 3.82080132, perplexity = 45.64076671\n",
      "\n",
      "Epoch training time: 8.083932399749756\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 28\n",
      "train_loss = 3.98366391, perplexity = 53.71347561\n",
      "validation_loss = 3.69632455, perplexity = 40.29891526\n",
      "\n",
      "Epoch training time: 8.095327377319336\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 29\n",
      "train_loss = 3.90852322, perplexity = 49.82531671\n",
      "validation_loss = 3.60310981, perplexity = 36.71222507\n",
      "\n",
      "Epoch training time: 8.114273309707642\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 30\n",
      "train_loss = 3.82859744, perplexity = 45.99797808\n",
      "validation_loss = 3.40865445, perplexity = 30.22454810\n",
      "\n",
      "Epoch training time: 8.132064819335938\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 31\n",
      "train_loss = 3.75354608, perplexity = 42.67213281\n",
      "validation_loss = 3.25118054, perplexity = 25.82080453\n",
      "\n",
      "Epoch training time: 8.134792804718018\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 32\n",
      "train_loss = 3.68205309, perplexity = 39.72787543\n",
      "validation_loss = 3.12972161, perplexity = 22.86761256\n",
      "\n",
      "Epoch training time: 8.082986116409302\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 33\n",
      "train_loss = 3.60899189, perplexity = 36.92880583\n",
      "validation_loss = 3.02819907, perplexity = 20.65999196\n",
      "\n",
      "Epoch training time: 8.132154941558838\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 34\n",
      "train_loss = 3.54069932, perplexity = 34.49103087\n",
      "validation_loss = 2.94382418, perplexity = 18.98832241\n",
      "\n",
      "Epoch training time: 8.080881357192993\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 35\n",
      "train_loss = 3.47535310, perplexity = 32.30923495\n",
      "validation_loss = 2.88825924, perplexity = 17.96201482\n",
      "\n",
      "Epoch training time: 8.107237339019775\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 36\n",
      "train_loss = 3.41270103, perplexity = 30.34710201\n",
      "validation_loss = 2.81436977, perplexity = 16.68265850\n",
      "\n",
      "Epoch training time: 8.121668577194214\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 37\n",
      "train_loss = 3.34952876, perplexity = 28.48930514\n",
      "validation_loss = 2.72888817, perplexity = 15.31584897\n",
      "\n",
      "Epoch training time: 8.138681173324585\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 38\n",
      "train_loss = 3.29860380, perplexity = 27.07481055\n",
      "validation_loss = 2.68950691, perplexity = 14.72441368\n",
      "\n",
      "Epoch training time: 8.090052604675293\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 39\n",
      "train_loss = 3.24948562, perplexity = 25.77707719\n",
      "validation_loss = 2.60742852, perplexity = 13.56412606\n",
      "\n",
      "Epoch training time: 8.124874830245972\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 40\n",
      "train_loss = 3.19728380, perplexity = 24.46598526\n",
      "validation_loss = 2.55897990, perplexity = 12.92262827\n",
      "\n",
      "Epoch training time: 8.083817481994629\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 41\n",
      "train_loss = 3.15574904, perplexity = 23.47061082\n",
      "validation_loss = 2.50056425, perplexity = 12.18936985\n",
      "\n",
      "Epoch training time: 8.121915340423584\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 42\n",
      "train_loss = 3.10456459, perplexity = 22.29950745\n",
      "validation_loss = 2.45047914, perplexity = 11.59390051\n",
      "\n",
      "Epoch training time: 8.143832683563232\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 43\n",
      "train_loss = 3.06666214, perplexity = 21.47011875\n",
      "validation_loss = 2.39399462, perplexity = 10.95717638\n",
      "\n",
      "Epoch training time: 8.140995025634766\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 44\n",
      "train_loss = 3.03223075, perplexity = 20.74345455\n",
      "validation_loss = 2.37144891, perplexity = 10.71290309\n",
      "\n",
      "Epoch training time: 8.121266603469849\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 45\n",
      "train_loss = 2.98417506, perplexity = 19.77018632\n",
      "validation_loss = 2.29275698, perplexity = 9.90220024\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training time: 8.128581762313843\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 46\n",
      "train_loss = 2.94174491, perplexity = 18.94888152\n",
      "validation_loss = 2.27822024, perplexity = 9.75929573\n",
      "\n",
      "Epoch training time: 8.148298263549805\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 47\n",
      "train_loss = 2.91358507, perplexity = 18.42272702\n",
      "validation_loss = 2.22284269, perplexity = 9.23354173\n",
      "\n",
      "Epoch training time: 8.148903369903564\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 48\n",
      "train_loss = 2.87698229, perplexity = 17.76059590\n",
      "validation_loss = 2.22527506, perplexity = 9.25602846\n",
      "\n",
      "Epoch training time: 8.1575288772583\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 49\n",
      "train_loss = 2.84776381, perplexity = 17.24916630\n",
      "validation_loss = 2.13689162, perplexity = 8.47305915\n",
      "\n",
      "Epoch training time: 8.12691330909729\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 50\n",
      "train_loss = 2.81535885, perplexity = 16.69916714\n",
      "validation_loss = 2.13372996, perplexity = 8.44631250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.train(50, save_dir=\"tmp\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/epoch050_2.1337.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"the king is in the service, which I had brought his power to his hand, I will not be it. What is it a very man of the king of the world. But, sir, I see your very good of love. I will be it. I that you shall have not the good a poor good of a husband, which he is a hair a kind of the gold the suit of you, I am as the good will be it. You are not to the duke? what you have heard you are as much a very man and of a house of a man a good a of a house of a good good of a good and a most of the poor courtesy. I advise you, my name? for you say, and that you have found to him it. I I have a delight in a man's house to a name. If I have heard it. Fare you well. I tell me a tawdry-lace and with your good I see your good good and good good of the good poor of my life, for I had been it. I I am but the good I will see it. You are a hair old. the Bless to the duke? What you have made you as they are not as a very good and of the present time of that the good which I come to make a good and a face. I am not not of the duke. If it be not known, not? What news as he mammocked to this the duke. But I have heard the duke of it. What I know not you are in this a man, for you have covetous. But I am not it. Fare you well. What I do you say, and I do\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.sample(300, load_dir=\"tmp\", starter_word=\"the\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
