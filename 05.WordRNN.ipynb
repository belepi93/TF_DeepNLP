{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. WordRNN - RNN Language Model with Words\n",
    "\n",
    "We will construct a Language Model with Recurrent Neural Networks. In this notebook, we will only cover one-way RNN/GRU/LSTM language model but it will be possible to expand one way network with one layer to bi-directional deep recurrent neural networks. Also, it is also possible to use CNN with RNN in order to construct a language model. We will cover it in a near future.\n",
    "\n",
    "Also, We will do some funny sentence generating from tinyshakespeare dataset. Although the primary goal of WordRNN.py is to build a language model and train/test on a PTB dataset, I've added some methods for sentence generating. But it's not optimized for sentence generating since `train()` method requires valid set for evaluating currently, which is not needed for sampling. I'm planning to fix it but it's not in my priority.\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 8](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 9](http://web.stanford.edu/class/cs224n/lectures/lecture9.pdf)\n",
    "- [mkroutikov/tf-lstm-char-cnn](https://github.com/mkroutikov/tf-lstm-char-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WordRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(data_dir):\n",
    "    corpus = []\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            tmp_line = line.strip().split(' ')\n",
    "            if len(tmp_line) == 1:\n",
    "                continue\n",
    "            corpus.append(tmp_line)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Penn Tree Bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = read_corpus(\"data/ptb/ptb.train.txt\")\n",
    "valid_corpus = read_corpus(\"data/ptb/ptb.valid.txt\")\n",
    "test_corpus = read_corpus(\"data/ptb/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you modify some parts of RNN_LM.py, you will be able to use pretrained word embeddings for this model and compare the performance of the cases when you use pretrained embeddings or not. I'll use pretrained embedding in future models, but in this model i'll leave it to readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04152210\n"
     ]
    }
   ],
   "source": [
    "model = WordRNN.WordRNN(word_embedding_size=128,\n",
    "                        hidden_size=512,\n",
    "                        cell=\"LSTM\",\n",
    "                        num_unroll_steps=30,\n",
    "                        learning_rate=0.01,\n",
    "                        batch_size=64,\n",
    "                        num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_corpus(train_corpus, valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 9821968\n",
      "--------------------------------------------------------------------------------\n",
      "000200: 1 [00200/00484], train_loss/perplexity = 6.77542877/876.0549316 secs/batch = 0.0493\n",
      "000400: 1 [00400/00484], train_loss/perplexity = 6.61267948/744.4751587 secs/batch = 0.0534\n",
      "Epoch training time: 24.902023315429688\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 6.96030980, perflexity = 1053.96002249\n",
      "validation_loss = 9.05610900, perflexity = 8570.73696744\n",
      "\n",
      "000684: 2 [00200/00484], train_loss/perplexity = 7.03914165/1140.4083252 secs/batch = 0.0501\n",
      "000884: 2 [00400/00484], train_loss/perplexity = 6.64665222/770.2015381 secs/batch = 0.0510\n",
      "Epoch training time: 24.878760814666748\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 7.07707315, perflexity = 1184.49659557\n",
      "validation_loss = 6.71638093, perflexity = 825.82338824\n",
      "\n",
      "001168: 3 [00200/00484], train_loss/perplexity = 6.82536936/920.9165039 secs/batch = 0.0492\n",
      "001368: 3 [00400/00484], train_loss/perplexity = 6.62086964/750.5975342 secs/batch = 0.0498\n",
      "Epoch training time: 24.933013439178467\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 6.94839813, perflexity = 1041.48007864\n",
      "validation_loss = 6.77270751, perflexity = 873.67417835\n",
      "\n",
      "001652: 4 [00200/00484], train_loss/perplexity = 6.80754185/904.6443481 secs/batch = 0.0505\n",
      "001852: 4 [00400/00484], train_loss/perplexity = 6.32876873/560.4660645 secs/batch = 0.0575\n",
      "Epoch training time: 24.954912900924683\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 6.80011958, perflexity = 897.95466559\n",
      "validation_loss = 6.39100885, perflexity = 596.45801526\n",
      "\n",
      "002136: 5 [00200/00484], train_loss/perplexity = 6.49890804/664.4157104 secs/batch = 0.0493\n",
      "002336: 5 [00400/00484], train_loss/perplexity = 6.03153324/416.3529053 secs/batch = 0.0552\n",
      "Epoch training time: 25.72667956352234\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 6.50201767, perflexity = 666.48502511\n",
      "validation_loss = 6.08526864, perflexity = 439.33781856\n",
      "\n",
      "002620: 6 [00200/00484], train_loss/perplexity = 6.19413948/489.8697205 secs/batch = 0.0514\n",
      "002820: 6 [00400/00484], train_loss/perplexity = 5.74807072/313.5850830 secs/batch = 0.0494\n",
      "Epoch training time: 25.256876945495605\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 6.17907296, perflexity = 482.54441209\n",
      "validation_loss = 5.87982887, perflexity = 357.74801404\n",
      "\n",
      "003104: 7 [00200/00484], train_loss/perplexity = 5.94591522/382.1889954 secs/batch = 0.0570\n",
      "003304: 7 [00400/00484], train_loss/perplexity = 5.50134802/245.0220032 secs/batch = 0.0487\n",
      "Epoch training time: 25.208750247955322\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 5.93674635, perflexity = 378.70076498\n",
      "validation_loss = 5.71979859, perflexity = 304.84351824\n",
      "\n",
      "003588: 8 [00200/00484], train_loss/perplexity = 5.86296225/351.7646179 secs/batch = 0.0520\n",
      "003788: 8 [00400/00484], train_loss/perplexity = 5.35672092/212.0285492 secs/batch = 0.0517\n",
      "Epoch training time: 26.064840078353882\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 5.79785248, perflexity = 329.59099633\n",
      "validation_loss = 5.60568316, perflexity = 271.96766039\n",
      "\n",
      "004072: 9 [00200/00484], train_loss/perplexity = 5.78533649/325.4915466 secs/batch = 0.0600\n",
      "004272: 9 [00400/00484], train_loss/perplexity = 5.24489880/189.5966339 secs/batch = 0.0506\n",
      "Epoch training time: 26.07908535003662\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 5.69192353, perflexity = 296.46332825\n",
      "validation_loss = 5.52538190, perflexity = 250.98217079\n",
      "\n",
      "004556: 10 [00200/00484], train_loss/perplexity = 5.73982382/311.0096130 secs/batch = 0.0535\n",
      "004756: 10 [00400/00484], train_loss/perplexity = 5.16698933/175.3860168 secs/batch = 0.0519\n",
      "Epoch training time: 25.76278042793274\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 5.60595260, perflexity = 272.04094740\n",
      "validation_loss = 5.46342708, perflexity = 235.90450407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(10, save_dir=\"save/05_rnn_lm\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/05_rnn_lm/epoch010_5.4634.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 9821968\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 5.39853896, perplexity = 221.08316968\n",
      "test samples: 002688, time elapsed: 1.0095, time per one batch: 0.0240\n"
     ]
    }
   ],
   "source": [
    "model.test(test_corpus, load_dir=\"save/05_rnn_lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try sampling with PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/05_rnn_lm/epoch010_5.4634.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"stock and and the N years old the N to N million shares and a $ N million of N N of its stock market 's new york stock exchange composite trading\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(30, load_dir=\"save/05_rnn_lm\", starter_word=\"stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With tinyshakespeare Data\n",
    "We will use tinyshakespeare dataset to make more 'plausible' sentence with RNN. We will compare the result from this WordRNN to CharRNN in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = read_corpus(\"data/rnn/input.txt\")\n",
    "valid_corpus = read_corpus(\"data/rnn/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04152210\n"
     ]
    }
   ],
   "source": [
    "model2 = RNN_LM.RNN_LM(word_embedding_size=256,\n",
    "                      hidden_size=512,\n",
    "                      cell=\"LSTM\",\n",
    "                      num_unroll_steps=30,\n",
    "                      learning_rate=0.005,\n",
    "                      batch_size=64,\n",
    "                      num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit_to_corpus(train_corpus, valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 23236703\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch training time: 9.39647626876831\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 7.98915776, perflexity = 2948.81230240\n",
      "validation_loss = 7.65021506, perflexity = 2101.09739967\n",
      "\n",
      "Epoch training time: 9.324654579162598\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 7.95522100, perflexity = 2850.41818842\n",
      "validation_loss = 8.17054488, perflexity = 3535.26971930\n",
      "\n",
      "Epoch training time: 9.602049112319946\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 8.32056282, perflexity = 4107.47113733\n",
      "validation_loss = 8.22334928, perflexity = 3726.96416135\n",
      "\n",
      "Epoch training time: 9.568662881851196\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 8.63934223, perflexity = 5649.61243285\n",
      "validation_loss = 8.40822631, perflexity = 4483.80055992\n",
      "\n",
      "Epoch training time: 9.521775484085083\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 8.91304082, perflexity = 7428.21464531\n",
      "validation_loss = 8.44634310, perflexity = 4658.00767056\n",
      "\n",
      "Epoch training time: 9.520504713058472\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 8.96737046, perflexity = 7842.95111913\n",
      "validation_loss = 8.43618671, perflexity = 4610.93858261\n",
      "\n",
      "Epoch training time: 9.684244155883789\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 8.60402861, perflexity = 5453.58575079\n",
      "validation_loss = 7.59309434, perflexity = 1984.44456248\n",
      "\n",
      "Epoch training time: 9.296607971191406\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 7.46576838, perflexity = 1747.19754853\n",
      "validation_loss = 6.66331139, perflexity = 783.13992751\n",
      "\n",
      "Epoch training time: 9.315052509307861\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 6.84397869, perflexity = 938.21458617\n",
      "validation_loss = 6.41313385, perflexity = 609.80171522\n",
      "\n",
      "Epoch training time: 9.489429473876953\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 6.54176048, perflexity = 693.50640642\n",
      "validation_loss = 6.24992420, perflexity = 517.97356044\n",
      "\n",
      "Epoch training time: 9.764513492584229\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 11\n",
      "train_loss = 6.34643111, perflexity = 570.45318793\n",
      "validation_loss = 6.06770991, perflexity = 431.69093958\n",
      "\n",
      "Epoch training time: 9.614114999771118\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 12\n",
      "train_loss = 6.18127257, perflexity = 483.60698988\n",
      "validation_loss = 5.91757707, perflexity = 371.51047686\n",
      "\n",
      "Epoch training time: 9.730756044387817\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 13\n",
      "train_loss = 6.04588247, perflexity = 422.37032118\n",
      "validation_loss = 5.81148261, perflexity = 334.11412108\n",
      "\n",
      "Epoch training time: 9.857567071914673\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 14\n",
      "train_loss = 5.92011499, perflexity = 372.45453841\n",
      "validation_loss = 5.69066023, perflexity = 296.08904422\n",
      "\n",
      "Epoch training time: 9.880003929138184\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 15\n",
      "train_loss = 5.80598014, perflexity = 332.28071397\n",
      "validation_loss = 5.57323801, perflexity = 263.28523993\n",
      "\n",
      "Epoch training time: 9.69536542892456\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 16\n",
      "train_loss = 5.70707634, perflexity = 300.98978755\n",
      "validation_loss = 5.48009167, perflexity = 239.86869524\n",
      "\n",
      "Epoch training time: 9.231891632080078\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 17\n",
      "train_loss = 5.61225842, perflexity = 273.76180893\n",
      "validation_loss = 5.39881016, perflexity = 221.14313569\n",
      "\n",
      "Epoch training time: 9.167171955108643\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 18\n",
      "train_loss = 5.52261356, perflexity = 250.28832727\n",
      "validation_loss = 5.31217546, perflexity = 202.79091216\n",
      "\n",
      "Epoch training time: 9.169432878494263\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 19\n",
      "train_loss = 5.44252318, perflexity = 231.02436459\n",
      "validation_loss = 5.19912212, perflexity = 181.11317654\n",
      "\n",
      "Epoch training time: 9.62962555885315\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 20\n",
      "train_loss = 5.35947254, perflexity = 212.61277256\n",
      "validation_loss = 5.13042295, perflexity = 169.08861822\n",
      "\n",
      "Epoch training time: 9.54783582687378\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 21\n",
      "train_loss = 5.28634343, perflexity = 197.61949409\n",
      "validation_loss = 5.06282618, perflexity = 158.03652500\n",
      "\n",
      "Epoch training time: 8.845979452133179\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 22\n",
      "train_loss = 5.21469574, perflexity = 183.95584232\n",
      "validation_loss = 4.98162238, perflexity = 145.71058845\n",
      "\n",
      "Epoch training time: 8.887490034103394\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 23\n",
      "train_loss = 5.13919878, perflexity = 170.57904293\n",
      "validation_loss = 4.93533166, perflexity = 139.11927477\n",
      "\n",
      "Epoch training time: 9.075692653656006\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 24\n",
      "train_loss = 5.06572914, perflexity = 158.49596524\n",
      "validation_loss = 4.85674041, perflexity = 128.60432119\n",
      "\n",
      "Epoch training time: 8.820647954940796\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 25\n",
      "train_loss = 5.00252480, perflexity = 148.78834659\n",
      "validation_loss = 4.79568578, perflexity = 120.98732359\n",
      "\n",
      "Epoch training time: 8.914322137832642\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 26\n",
      "train_loss = 4.93272186, perflexity = 138.75667469\n",
      "validation_loss = 4.70336647, perflexity = 110.31792989\n",
      "\n",
      "Epoch training time: 8.860749244689941\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 27\n",
      "train_loss = 4.85946297, perflexity = 128.95493114\n",
      "validation_loss = 4.65933199, perflexity = 105.56553951\n",
      "\n",
      "Epoch training time: 8.925893306732178\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 28\n",
      "train_loss = 4.78817439, perflexity = 120.08194573\n",
      "validation_loss = 4.55633234, perflexity = 95.23355456\n",
      "\n",
      "Epoch training time: 8.911633729934692\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 29\n",
      "train_loss = 4.72523168, perflexity = 112.75661931\n",
      "validation_loss = 4.46745047, perflexity = 87.13428852\n",
      "\n",
      "Epoch training time: 9.093775033950806\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 30\n",
      "train_loss = 4.65440166, perflexity = 105.04634732\n",
      "validation_loss = 4.40615732, perflexity = 81.95393499\n",
      "\n",
      "Epoch training time: 8.872097969055176\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 31\n",
      "train_loss = 4.57847668, perflexity = 97.36596124\n",
      "validation_loss = 4.31611510, perflexity = 74.89709494\n",
      "\n",
      "Epoch training time: 8.940861463546753\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 32\n",
      "train_loss = 4.50928723, perflexity = 90.85703562\n",
      "validation_loss = 4.23887959, perflexity = 69.33012986\n",
      "\n",
      "Epoch training time: 8.872238397598267\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 33\n",
      "train_loss = 4.43685325, perflexity = 84.50859528\n",
      "validation_loss = 4.19993117, perflexity = 66.68174127\n",
      "\n",
      "Epoch training time: 8.902621746063232\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 34\n",
      "train_loss = 4.36785156, perflexity = 78.87399329\n",
      "validation_loss = 4.15045458, perflexity = 63.46284273\n",
      "\n",
      "Epoch training time: 8.891710996627808\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 35\n",
      "train_loss = 4.30656061, perflexity = 74.18489908\n",
      "validation_loss = 3.98745142, perflexity = 53.91730176\n",
      "\n",
      "Epoch training time: 8.843892812728882\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 36\n",
      "train_loss = 4.23788511, perflexity = 69.26121707\n",
      "validation_loss = 3.88742143, perflexity = 48.78492880\n",
      "\n",
      "Epoch training time: 8.877688646316528\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 37\n",
      "train_loss = 4.17126453, perflexity = 64.79733858\n",
      "validation_loss = 3.77311971, perflexity = 43.51560958\n",
      "\n",
      "Epoch training time: 9.030961036682129\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 38\n",
      "train_loss = 4.09803994, perflexity = 60.22213306\n",
      "validation_loss = 3.69133510, perflexity = 40.09834648\n",
      "\n",
      "Epoch training time: 9.199415445327759\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 39\n",
      "train_loss = 4.03712516, perflexity = 56.66321053\n",
      "validation_loss = 3.61808850, perflexity = 37.26626524\n",
      "\n",
      "Epoch training time: 8.902214765548706\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 40\n",
      "train_loss = 3.99584429, perflexity = 54.37172665\n",
      "validation_loss = 3.53304447, perflexity = 34.22801545\n",
      "\n",
      "Epoch training time: 8.90479564666748\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 41\n",
      "train_loss = 3.92872524, perflexity = 50.84212497\n",
      "validation_loss = 3.50384146, perflexity = 33.24290822\n",
      "\n",
      "Epoch training time: 8.916681289672852\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 42\n",
      "train_loss = 3.88597213, perflexity = 48.71427585\n",
      "validation_loss = 3.46113280, perflexity = 31.85303927\n",
      "\n",
      "Epoch training time: 8.866676330566406\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 43\n",
      "train_loss = 3.83505926, perflexity = 46.29617131\n",
      "validation_loss = 3.45915252, perflexity = 31.79002368\n",
      "\n",
      "Epoch training time: 8.92579174041748\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 44\n",
      "train_loss = 3.80463381, perflexity = 44.90880179\n",
      "validation_loss = 3.36289565, perflexity = 28.87267508\n",
      "\n",
      "Epoch training time: 8.835824489593506\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 45\n",
      "train_loss = 3.74688478, perflexity = 42.38882575\n",
      "validation_loss = 3.30338033, perflexity = 27.20444375\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training time: 8.92335033416748\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 46\n",
      "train_loss = 3.69393020, perflexity = 40.20254091\n",
      "validation_loss = 3.29468464, perflexity = 26.96890788\n",
      "\n",
      "Epoch training time: 8.883531332015991\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 47\n",
      "train_loss = 3.65591117, perflexity = 38.70276994\n",
      "validation_loss = 3.28405757, perflexity = 26.68382477\n",
      "\n",
      "Epoch training time: 8.856118440628052\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 48\n",
      "train_loss = 3.61137724, perflexity = 37.01699891\n",
      "validation_loss = 3.16153612, perflexity = 23.60683112\n",
      "\n",
      "Epoch training time: 8.85993766784668\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 49\n",
      "train_loss = 3.55849476, perflexity = 35.11030788\n",
      "validation_loss = 3.11998596, perflexity = 22.64606180\n",
      "\n",
      "Epoch training time: 8.856158256530762\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 50\n",
      "train_loss = 3.52563918, perflexity = 33.97548314\n",
      "validation_loss = 3.00872398, perflexity = 20.26152931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.train(50, save_dir=\"tmp\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/epoch050_3.0087.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the gods know I would have not be a good man, my lord, my lord, you have the king. I am not at the duke? You are not so. You will not be so. good villain! You must be whipt. I have done to a husband as you sleep? you shall not'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.sample(50, load_dir=\"tmp\", starter_word=\"the\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
