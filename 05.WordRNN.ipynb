{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. WordRNN - RNN Language Model with Words\n",
    "\n",
    "We will construct a Language Model with Recurrent Neural Networks. In this notebook, we will only cover one-way RNN/GRU/LSTM language model but it will be possible to expand one way network with one layer to bi-directional deep recurrent neural networks. Also, it is also possible to use CNN with RNN in order to construct a language model. We will cover it in a near future.\n",
    "\n",
    "Also, We will do some funny sentence generating from tinyshakespeare dataset. Although the primary goal of WordRNN.py is to build a language model and train/test on a PTB dataset, I've added some methods for sentence generating. But it's not optimized for sentence generating since `train()` method requires valid set for evaluating currently, which is not needed for sampling. I'm planning to fix it but it's not in my priority.\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 8](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 9](http://web.stanford.edu/class/cs224n/lectures/lecture9.pdf)\n",
    "- [mkroutikov/tf-lstm-char-cnn](https://github.com/mkroutikov/tf-lstm-char-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WordRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(data_dir):\n",
    "    corpus = []\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            tmp_line = line.strip().split(' ')\n",
    "            if len(tmp_line) == 1:\n",
    "                continue\n",
    "            corpus.append(tmp_line)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Penn Tree Bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = read_corpus(\"data/rnnlm_datasets/ptb/train.txt\")\n",
    "valid_corpus = read_corpus(\"data/rnnlm_datasets/ptb/valid.txt\")\n",
    "test_corpus = read_corpus(\"data/rnnlm_datasets/ptb/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you modify some parts of RNN_LM.py, you will be able to use pretrained word embeddings for this model and compare the performance of the cases when you use pretrained embeddings or not. I'll use pretrained embedding in future models, but in this model i'll leave it to readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04152210\n"
     ]
    }
   ],
   "source": [
    "model = WordRNN.WordRNN(word_embedding_size=128,\n",
    "                        hidden_size=512,\n",
    "                        cell=\"LSTM\",\n",
    "                        num_unroll_steps=30,\n",
    "                        learning_rate=0.001,\n",
    "                        batch_size=64,\n",
    "                        num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_to_corpus(train_corpus, valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 9821327\n",
      "--------------------------------------------------------------------------------\n",
      "000200: 1 [00200/00484], train_loss/perplexity = 6.63781738/763.4268799 secs/batch = 0.0433\n",
      "000400: 1 [00400/00484], train_loss/perplexity = 6.59713650/732.9932251 secs/batch = 0.0452\n",
      "Epoch training time: 21.694878101348877\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 6.74412218, perplexity = 849.05348673\n",
      "validation_loss = 6.65829144, perplexity = 779.21845786\n",
      "\n",
      "000684: 2 [00200/00484], train_loss/perplexity = 6.55697107/704.1356812 secs/batch = 0.0490\n",
      "000884: 2 [00400/00484], train_loss/perplexity = 6.24998665/518.0059204 secs/batch = 0.0464\n",
      "Epoch training time: 22.208510875701904\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 6.50202824, perplexity = 666.49206807\n",
      "validation_loss = 6.18816230, perplexity = 486.95041552\n",
      "\n",
      "001168: 3 [00200/00484], train_loss/perplexity = 5.91213751/369.4951172 secs/batch = 0.0445\n",
      "001368: 3 [00400/00484], train_loss/perplexity = 5.80315971/331.3448486 secs/batch = 0.0443\n",
      "Epoch training time: 22.022931337356567\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 5.97342562, perplexity = 392.84911856\n",
      "validation_loss = 5.80011393, perplexity = 330.33719273\n",
      "\n",
      "001652: 4 [00200/00484], train_loss/perplexity = 5.48592186/241.2712555 secs/batch = 0.0452\n",
      "001852: 4 [00400/00484], train_loss/perplexity = 5.51087189/247.3667145 secs/batch = 0.0462\n",
      "Epoch training time: 21.978148937225342\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 5.60809143, perplexity = 272.62342007\n",
      "validation_loss = 5.47409211, perplexity = 238.43389619\n",
      "\n",
      "002136: 5 [00200/00484], train_loss/perplexity = 5.26473379/193.3948212 secs/batch = 0.0453\n",
      "002336: 5 [00400/00484], train_loss/perplexity = 5.37151480/215.1885834 secs/batch = 0.0447\n",
      "Epoch training time: 21.84995412826538\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 5.41431231, perplexity = 224.59803906\n",
      "validation_loss = 5.34888252, perplexity = 210.37307972\n",
      "\n",
      "002620: 6 [00200/00484], train_loss/perplexity = 5.11114168/165.8596039 secs/batch = 0.0463\n",
      "002820: 6 [00400/00484], train_loss/perplexity = 5.18174458/177.9930573 secs/batch = 0.0439\n",
      "Epoch training time: 21.843682289123535\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 5.25246955, perplexity = 191.03746362\n",
      "validation_loss = 5.26371592, perplexity = 193.19806761\n",
      "\n",
      "003104: 7 [00200/00484], train_loss/perplexity = 4.99582338/147.7945862 secs/batch = 0.0455\n",
      "003304: 7 [00400/00484], train_loss/perplexity = 5.08384180/161.3928986 secs/batch = 0.0443\n",
      "Epoch training time: 21.85898184776306\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 5.14679733, perplexity = 171.88013303\n",
      "validation_loss = 5.26200508, perplexity = 192.86781891\n",
      "\n",
      "003588: 8 [00200/00484], train_loss/perplexity = 4.92192745/137.2669373 secs/batch = 0.0443\n",
      "003788: 8 [00400/00484], train_loss/perplexity = 5.06355238/158.1513367 secs/batch = 0.0453\n",
      "Epoch training time: 22.369683265686035\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 5.09013887, perplexity = 162.41241435\n",
      "validation_loss = 5.16544279, perplexity = 175.11498109\n",
      "\n",
      "004072: 9 [00200/00484], train_loss/perplexity = 4.83709431/126.1024017 secs/batch = 0.0441\n",
      "004272: 9 [00400/00484], train_loss/perplexity = 4.98855400/146.7241058 secs/batch = 0.0446\n",
      "Epoch training time: 22.048228979110718\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 5.01354346, perplexity = 150.43685951\n",
      "validation_loss = 5.13281436, perplexity = 169.49346257\n",
      "\n",
      "004556: 10 [00200/00484], train_loss/perplexity = 4.74236107/114.7047043 secs/batch = 0.0446\n",
      "004756: 10 [00400/00484], train_loss/perplexity = 4.94399452/140.3296814 secs/batch = 0.0445\n",
      "Epoch training time: 21.889522552490234\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 4.94720614, perplexity = 140.78109166\n",
      "validation_loss = 5.09365027, perplexity = 162.98371140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(10, save_dir=\"save/05_rnn_lm\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/05_rnn_lm/epoch010_5.0937.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 9821327\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 5.17648795, perplexity = 177.05987379\n",
      "test samples: 002688, time elapsed: 0.7923, time per one batch: 0.0189\n"
     ]
    }
   ],
   "source": [
    "model.test(test_corpus, load_dir=\"save/05_rnn_lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try sampling with PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/05_rnn_lm/epoch010_5.0937.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"stock rothschilds fleischmann benefit-seeking a N million 's a the N to the and the the a in N of the the a the the a the the the N in N\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(30, load_dir=\"save/05_rnn_lm\", starter_word=\"stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With tinyshakespeare Data\n",
    "We will use tinyshakespeare dataset for sampling. We will compare the result from this WordRNN to CharRNN in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = read_corpus(\"data/rnn/input.txt\")\n",
    "valid_corpus = read_corpus(\"data/rnn/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04152210\n"
     ]
    }
   ],
   "source": [
    "model2 = WordRNN.WordRNN(word_embedding_size=256,\n",
    "                        hidden_size=512,\n",
    "                        cell=\"LSTM\",\n",
    "                        num_unroll_steps=30,\n",
    "                        learning_rate=0.005,\n",
    "                        batch_size=64,\n",
    "                        num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit_to_corpus(train_corpus, valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 23236703\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch training time: 8.462949514389038\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 7.86568061, perplexity = 2606.28368003\n",
      "validation_loss = 7.60973909, perplexity = 2017.75158543\n",
      "\n",
      "Epoch training time: 9.069615602493286\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 7.95595207, perplexity = 2852.50281576\n",
      "validation_loss = 7.78833448, perplexity = 2412.29649810\n",
      "\n",
      "Epoch training time: 9.387078523635864\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 8.27326415, perplexity = 3917.71616978\n",
      "validation_loss = 7.98781574, perplexity = 2944.85758807\n",
      "\n",
      "Epoch training time: 10.45639181137085\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 8.57017626, perplexity = 5272.05894093\n",
      "validation_loss = 8.18246222, perplexity = 3577.65279150\n",
      "\n",
      "Epoch training time: 10.32453727722168\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 8.77075658, perplexity = 6443.04531117\n",
      "validation_loss = 8.26412060, perplexity = 3882.05757623\n",
      "\n",
      "Epoch training time: 10.682037830352783\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 8.80463468, perplexity = 6665.06295349\n",
      "validation_loss = 8.21938324, perplexity = 3712.21213142\n",
      "\n",
      "Epoch training time: 10.971947193145752\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 8.75356989, perplexity = 6333.25683493\n",
      "validation_loss = 8.09160210, perplexity = 3266.91731936\n",
      "\n",
      "Epoch training time: 10.156201839447021\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 8.58524284, perplexity = 5352.09225261\n",
      "validation_loss = 7.64456671, perplexity = 2089.26312853\n",
      "\n",
      "Epoch training time: 9.153602361679077\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 7.93992359, perplexity = 2807.14599630\n",
      "validation_loss = 7.59581709, perplexity = 1989.85508447\n",
      "\n",
      "Epoch training time: 9.18687129020691\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 7.94602091, perplexity = 2824.31435342\n",
      "validation_loss = 7.23540584, perplexity = 1387.70396360\n",
      "\n",
      "Epoch training time: 8.61947250366211\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 11\n",
      "train_loss = 7.09467774, perplexity = 1205.53381067\n",
      "validation_loss = 6.41423423, perplexity = 610.47309805\n",
      "\n",
      "Epoch training time: 9.378437519073486\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 12\n",
      "train_loss = 6.59479893, perplexity = 731.28183362\n",
      "validation_loss = 6.19695007, perplexity = 491.24847835\n",
      "\n",
      "Epoch training time: 9.061574697494507\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 13\n",
      "train_loss = 6.36894749, perplexity = 583.44342289\n",
      "validation_loss = 6.01161356, perplexity = 408.14135070\n",
      "\n",
      "Epoch training time: 10.229979991912842\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 14\n",
      "train_loss = 6.19109491, perplexity = 488.38054545\n",
      "validation_loss = 5.85949242, perplexity = 350.54616903\n",
      "\n",
      "Epoch training time: 8.905138731002808\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 15\n",
      "train_loss = 6.04347915, perplexity = 421.35645029\n",
      "validation_loss = 5.73693816, perplexity = 310.11343942\n",
      "\n",
      "Epoch training time: 8.936103343963623\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 16\n",
      "train_loss = 5.91306351, perplexity = 369.83742376\n",
      "validation_loss = 5.59123085, perplexity = 268.06536576\n",
      "\n",
      "Epoch training time: 9.225392818450928\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 17\n",
      "train_loss = 5.79137546, perplexity = 327.46312594\n",
      "validation_loss = 5.49142160, perplexity = 242.60184582\n",
      "\n",
      "Epoch training time: 8.742834091186523\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 18\n",
      "train_loss = 5.67481680, perplexity = 291.43494325\n",
      "validation_loss = 5.36768425, perplexity = 214.36587453\n",
      "\n",
      "Epoch training time: 8.903193473815918\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 19\n",
      "train_loss = 5.57451921, perplexity = 263.62277819\n",
      "validation_loss = 5.26778686, perplexity = 193.98616936\n",
      "\n",
      "Epoch training time: 8.780484199523926\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 20\n",
      "train_loss = 5.48760469, perplexity = 241.67761968\n",
      "validation_loss = 5.19923353, perplexity = 181.13335498\n",
      "\n",
      "Epoch training time: 8.458096027374268\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 21\n",
      "train_loss = 5.40580735, perplexity = 222.69594177\n",
      "validation_loss = 5.10374904, perplexity = 164.63798559\n",
      "\n",
      "Epoch training time: 8.574681997299194\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 22\n",
      "train_loss = 5.31299977, perplexity = 202.95814316\n",
      "validation_loss = 5.01536615, perplexity = 150.71130920\n",
      "\n",
      "Epoch training time: 8.544999361038208\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 23\n",
      "train_loss = 5.22684315, perplexity = 186.20405596\n",
      "validation_loss = 4.91044179, perplexity = 135.69935167\n",
      "\n",
      "Epoch training time: 8.97533917427063\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 24\n",
      "train_loss = 5.15096842, perplexity = 172.59855803\n",
      "validation_loss = 4.85404359, perplexity = 128.25796541\n",
      "\n",
      "Epoch training time: 9.155365228652954\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 25\n",
      "train_loss = 5.08003980, perplexity = 160.78045507\n",
      "validation_loss = 4.77078499, perplexity = 118.01184331\n",
      "\n",
      "Epoch training time: 8.99975299835205\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 26\n",
      "train_loss = 5.00508414, perplexity = 149.16963385\n",
      "validation_loss = 4.66413568, perplexity = 106.07386362\n",
      "\n",
      "Epoch training time: 8.703262329101562\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 27\n",
      "train_loss = 4.93354654, perplexity = 138.87115197\n",
      "validation_loss = 4.58383748, perplexity = 97.88932311\n",
      "\n",
      "Epoch training time: 8.348475217819214\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 28\n",
      "train_loss = 4.85256240, perplexity = 128.06813191\n",
      "validation_loss = 4.50251096, perplexity = 90.24344461\n",
      "\n",
      "Epoch training time: 8.995254755020142\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 29\n",
      "train_loss = 4.78121739, perplexity = 119.24943480\n",
      "validation_loss = 4.40178705, perplexity = 81.59655531\n",
      "\n",
      "Epoch training time: 9.059787511825562\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 30\n",
      "train_loss = 4.69956548, perplexity = 109.89940864\n",
      "validation_loss = 4.30502527, perplexity = 74.07108700\n",
      "\n",
      "Epoch training time: 9.699107885360718\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 31\n",
      "train_loss = 4.63411038, perplexity = 102.93630314\n",
      "validation_loss = 4.22952393, perplexity = 68.68452547\n",
      "\n",
      "Epoch training time: 9.536523818969727\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 32\n",
      "train_loss = 4.56262062, perplexity = 95.83429624\n",
      "validation_loss = 4.15072314, perplexity = 63.47988886\n",
      "\n",
      "Epoch training time: 10.18520712852478\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 33\n",
      "train_loss = 4.49893240, perplexity = 89.92108054\n",
      "validation_loss = 4.05743006, perplexity = 57.82551209\n",
      "\n",
      "Epoch training time: 9.36876368522644\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 34\n",
      "train_loss = 4.42916477, perplexity = 83.86134426\n",
      "validation_loss = 3.98598859, perplexity = 53.83848727\n",
      "\n",
      "Epoch training time: 8.505663394927979\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 35\n",
      "train_loss = 4.35627458, perplexity = 77.96613612\n",
      "validation_loss = 3.89985784, perplexity = 49.39542664\n",
      "\n",
      "Epoch training time: 8.774072170257568\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 36\n",
      "train_loss = 4.31024159, perplexity = 74.45847506\n",
      "validation_loss = 3.80162120, perplexity = 44.77371286\n",
      "\n",
      "Epoch training time: 8.674591541290283\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 37\n",
      "train_loss = 4.22701209, perplexity = 68.51221754\n",
      "validation_loss = 3.75207484, perplexity = 42.60939793\n",
      "\n",
      "Epoch training time: 9.317301511764526\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 38\n",
      "train_loss = 4.16897862, perplexity = 64.64938686\n",
      "validation_loss = 3.63576914, perplexity = 37.93101606\n",
      "\n",
      "Epoch training time: 9.239099025726318\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 39\n",
      "train_loss = 4.09025177, perplexity = 59.75493410\n",
      "validation_loss = 3.56942554, perplexity = 35.49619614\n",
      "\n",
      "Epoch training time: 9.198012113571167\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 40\n",
      "train_loss = 4.03616075, perplexity = 56.60859046\n",
      "validation_loss = 3.48386050, perplexity = 32.58527492\n",
      "\n",
      "Epoch training time: 9.204922914505005\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 41\n",
      "train_loss = 3.97482588, perplexity = 53.24084567\n",
      "validation_loss = 3.37633729, perplexity = 29.26339114\n",
      "\n",
      "Epoch training time: 8.76886510848999\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 42\n",
      "train_loss = 3.91681975, perplexity = 50.24041312\n",
      "validation_loss = 3.30757038, perplexity = 27.31867088\n",
      "\n",
      "Epoch training time: 8.67212724685669\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 43\n",
      "train_loss = 3.86473464, perplexity = 47.69061574\n",
      "validation_loss = 3.23516140, perplexity = 25.41047273\n",
      "\n",
      "Epoch training time: 8.747432947158813\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 44\n",
      "train_loss = 3.80832134, perplexity = 45.07471004\n",
      "validation_loss = 3.18013767, perplexity = 24.05006418\n",
      "\n",
      "Epoch training time: 8.80184292793274\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 45\n",
      "train_loss = 3.75640999, perplexity = 42.79451724\n",
      "validation_loss = 3.13066832, perplexity = 22.88927173\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training time: 9.158948421478271\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 46\n",
      "train_loss = 3.69821189, perplexity = 40.37504491\n",
      "validation_loss = 3.06578425, perplexity = 21.45127849\n",
      "\n",
      "Epoch training time: 8.781265020370483\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 47\n",
      "train_loss = 3.64910029, perplexity = 38.44006570\n",
      "validation_loss = 3.01453912, perplexity = 20.37969623\n",
      "\n",
      "Epoch training time: 8.434995174407959\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 48\n",
      "train_loss = 3.59858961, perplexity = 36.54665287\n",
      "validation_loss = 2.96528140, perplexity = 19.40016176\n",
      "\n",
      "Epoch training time: 8.637258291244507\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 49\n",
      "train_loss = 3.57007013, perplexity = 35.51908406\n",
      "validation_loss = 2.94526690, perplexity = 19.01573702\n",
      "\n",
      "Epoch training time: 8.625694990158081\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 50\n",
      "train_loss = 3.53479892, perplexity = 34.28811923\n",
      "validation_loss = 2.89230615, perplexity = 18.03485270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.train(50, save_dir=\"tmp\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/epoch050_2.8923.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"the man that is not so too! What O, I have the king. I am not for the time of marriage I will be a king. A I will be the king. A duke's matter, surely: I am not the king. How I pray you, sir, be not a word. My lord, I have not a king; for it is I love. What I am not known my very good time I will not be a love. When I have been a woman in a king. When I am a gentleman? Let me be a king. But I am not too much: or a' and play to the king. How I have a cause with me. How I will tell you so, I am a poor man and a very house he hath a very piece of the north, he had he had it is the very thing and a king's son should be I sent it. A mother, and a very man and the very house and old old house of a war I pray. I come to the poor house of his old looks, come to the good man and a very house and a old man I make, my good good good and and the maid is the very house of his life, and I have it home to his good and the man I see the good of a old man and the war I pray. I pray thee, I had a woman is a very man I prize the very good of a time he would have I am the king. What DUKE VINCENTIO: I kneel'd to the good and good a good man was the king. How I am a very fellow, nor I have heard you good: What it would not be a very good\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.sample(300, load_dir=\"tmp\", starter_word=\"the\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
