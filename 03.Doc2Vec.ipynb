{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Doc2Vec - Distributed Representations of Sentences and Documents\n",
    "\n",
    "### References\n",
    "- [Distributed Representations of Sentences and Documents, Le et al. 2014](https://arxiv.org/pdf/1405.4053.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Doc2Vec\n",
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Methods\n",
    "Define utility methods for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(texts, stops):\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    # remove empty strings\n",
    "    texts = [x for x in texts if x != '']\n",
    "    \n",
    "    return(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Corpus\n",
    "We will use [Amazon Food Review Corpus](https://www.kaggle.com/snap/amazon-fine-food-reviews). If we have already normalized and saved corpus with pickle, load the data. If not, load raw data and normalize the corpus and save it. This will takes some time since these codes are not optimized for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"data/amazon_food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found amazon_corpus.pkl. Loading Corpus..\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(DATA_DIR,\"amazon_corpus.pkl\")):\n",
    "    print(\"Found amazon_corpus.pkl. Loading Corpus..\")\n",
    "    with open(os.path.join(DATA_DIR,\"amazon_corpus.pkl\"), \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print(\"No amazon_corpus.pkl. Preprocessing Corpus..\")\n",
    "    with open(os.path.join(DATA_DIR, \"Reviews.csv\"), 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)\n",
    "        corpus = [line[-1] for line in reader]\n",
    "\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    corpus = normalize_corpus(corpus, stops)\n",
    "\n",
    "    corpus = [line.split(' ') for line in corpus]\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, \"amazon_corpus.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387867"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten(corpus[:10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2Vec.Doc2VecModel(100, 100, 5, max_vocab_size=100000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "doc2vec.fit_to_corpus(corpus[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TensorBoard summaries to log/doc2vec\n",
      "Saving TensorFlow models to save/doc2vec\n",
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 8092463\n",
      "Total number of batches: 758\n",
      "--------------------------------------------------------------------------------\n",
      "step: 1000, epoch:2, time/batch: 0.005252, avg_loss: 35.83\n",
      "step: 2000, epoch:3, time/batch: 0.005116, avg_loss: 30.03\n",
      "step: 3000, epoch:4, time/batch: 0.005247, avg_loss: 25.68\n",
      "step: 4000, epoch:6, time/batch: 0.005416, avg_loss: 22.43\n",
      "step: 5000, epoch:7, time/batch: 0.00544, avg_loss: 19.94\n",
      "Saved summaries at step 5000\n",
      "Saved a model at step 5000\n",
      "step: 6000, epoch:8, time/batch: 0.004783, avg_loss: 19.33\n",
      "step: 7000, epoch:10, time/batch: 0.00462, avg_loss: 17.39\n",
      "step: 8000, epoch:11, time/batch: 0.004677, avg_loss: 15.63\n",
      "step: 9000, epoch:12, time/batch: 0.005184, avg_loss: 15.8\n",
      "step: 10000, epoch:14, time/batch: 0.005299, avg_loss: 14.92\n",
      "Saved summaries at step 10000\n",
      "Saved a model at step 10000\n",
      "step: 11000, epoch:15, time/batch: 0.004643, avg_loss: 13.72\n",
      "step: 12000, epoch:16, time/batch: 0.004568, avg_loss: 13.27\n",
      "step: 13000, epoch:18, time/batch: 0.004892, avg_loss: 12.95\n",
      "step: 14000, epoch:19, time/batch: 0.005321, avg_loss: 12.82\n",
      "step: 15000, epoch:20, time/batch: 0.005313, avg_loss: 12.78\n",
      "Saved summaries at step 15000\n",
      "Saved a model at step 15000\n"
     ]
    }
   ],
   "source": [
    "doc2vec.train(20, log_dir=\"log/03_doc2vec\", save_dir=\"save/03_doc2vec\", print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load original corpus for test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"Reviews.csv\"), 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    corpus = [line[-1] for line in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if these results are acceptable. Maybe we need some hyperparameter tunings and more data. Since our main point is not in getting good result but in implementing models in a consistent way, I'll leave the process of getting good results to readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_similarity(target, model, corpus):\n",
    "    print(\"TARGET SENTENCE: {}\".format(corpus[target]))\n",
    "    target_V = model.embedding_for(target)\n",
    "    \n",
    "    similarities = []\n",
    "    for doc in range(model.corpus_size):\n",
    "        if doc == target: continue\n",
    "        \n",
    "        vector = model.embedding_for(doc)\n",
    "        cosine_sim = 1 - spatial.distance.cosine(target_V, vector)\n",
    "        similarities.append([corpus[doc].strip(), cosine_sim])\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET SENTENCE: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.',\n",
       "  0.49768173694610596],\n",
       " [\"My cats have been happily eating Felidae Platinum for more than two years. I just got a new bag and the shape of the food is different. They tried the new food when I first put it in their bowls and now the bowls sit full and the kitties will not touch the food. I've noticed similar reviews related to formula changes in the past. Unfortunately, I now need to find a new food that my cats will eat.\",\n",
       "  0.4471796452999115],\n",
       " ['I am very satisfied with my Twizzler purchase.  I shared these with others and we have all enjoyed them.  I will definitely be ordering more.',\n",
       "  0.4460362493991852],\n",
       " ['This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!',\n",
       "  0.4460066854953766],\n",
       " [\"One of my boys needed to lose some weight and the other didn't.  I put this food on the floor for the chubby guy, and the protein-rich, no by-product food up higher where only my skinny boy can jump.  The higher food sits going stale.  They both really go for this food.  And my chubby boy has been losing about an ounce a week.\",\n",
       "  0.4018159806728363]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_similarity(0, doc2vec, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec.generate_tsne('log/03_doc2vec/fig.png', size=(30,20), doc_count=50, corpus=corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
