{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Doc2Vec - Distributed Representations of Sentences and Documents\n",
    "\n",
    "### References\n",
    "- [Distributed Representations of Sentences and Documents, Le et al. 2014](https://arxiv.org/pdf/1405.4053.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Doc2Vec\n",
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Methods\n",
    "Define utility methods for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(texts, stops):\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    # remove empty strings\n",
    "    texts = [x for x in texts if x != '']\n",
    "    \n",
    "    return(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Normalize Corpus\n",
    "We will use [Amazon Food Review Corpus](https://www.kaggle.com/snap/amazon-fine-food-reviews). If we have already normalized and saved corpus with pickle, load the data. If not, load raw data and normalize the corpus and save it. This will takes some time since these codes are not optimized for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found amazon_corpus.pkl. Loading Corpus..\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(\"data\",\"amazon_corpus.pkl\")):\n",
    "    print(\"Found amazon_corpus.pkl. Loading Corpus..\")\n",
    "    with open(os.path.join(\"data\",\"amazon_corpus.pkl\"), \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print(\"No amazon_corpus.pkl. Preprocessing Corpus..\")\n",
    "    with open(os.path.join(\"data\", \"Reviews.csv\"), 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)\n",
    "        corpus = [line[-1] for line in reader]\n",
    "\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    corpus = normalize_corpus(corpus, stops)\n",
    "\n",
    "    corpus = [line.split(' ') for line in corpus]\n",
    "    \n",
    "    with open(os.path.join(\"data\", \"amazon_corpus.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387867"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten(corpus[:10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2Vec.Doc2VecModel(100, 100, 5, max_vocab_size=100000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "doc2vec.fit_to_corpus(corpus[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TensorBoard summaries to log/doc2vec\n",
      "Saving TensorFlow models to save/doc2vec\n",
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 9448362\n",
      "Total number of batches: 758\n",
      "--------------------------------------------------------------------------------\n",
      "step: 1000, epoch:2, time/batch: 0.005517543077468872, avg_loss: 35.93056869506836\n",
      "step: 2000, epoch:3, time/batch: 0.005339736461639404, avg_loss: 29.108983993530273\n",
      "step: 3000, epoch:4, time/batch: 0.005398691892623902, avg_loss: 25.362348556518555\n",
      "step: 4000, epoch:6, time/batch: 0.005444626331329346, avg_loss: 20.517866134643555\n",
      "step: 5000, epoch:7, time/batch: 0.0053118314743042, avg_loss: 19.599010467529297\n",
      "Saved summaries at step 5000\n",
      "Saved a model at step 5000\n",
      "step: 6000, epoch:8, time/batch: 0.0050750067234039305, avg_loss: 19.227130889892578\n",
      "step: 7000, epoch:10, time/batch: 0.004860623836517334, avg_loss: 16.350727081298828\n",
      "step: 8000, epoch:11, time/batch: 0.005074711799621582, avg_loss: 16.42232894897461\n",
      "step: 9000, epoch:12, time/batch: 0.005370182991027832, avg_loss: 15.390006065368652\n",
      "step: 10000, epoch:14, time/batch: 0.005434684038162232, avg_loss: 14.159655570983887\n",
      "Saved summaries at step 10000\n",
      "Saved a model at step 10000\n",
      "step: 11000, epoch:15, time/batch: 0.0049892208576202395, avg_loss: 14.81084156036377\n",
      "step: 12000, epoch:16, time/batch: 0.0047200086116790775, avg_loss: 13.762849807739258\n",
      "step: 13000, epoch:18, time/batch: 0.004967283964157104, avg_loss: 13.824158668518066\n",
      "step: 14000, epoch:19, time/batch: 0.005366640567779541, avg_loss: 11.841497421264648\n",
      "step: 15000, epoch:20, time/batch: 0.005815485000610352, avg_loss: 12.143861770629883\n",
      "Saved summaries at step 15000\n",
      "Saved a model at step 15000\n"
     ]
    }
   ],
   "source": [
    "doc2vec.train(20, log_dir=\"log/doc2vec\", save_dir=\"save/doc2vec\", print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load original corpus for test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data\", \"Reviews.csv\"), 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    corpus = [line[-1] for line in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if these results are acceptable. Maybe we need some hyperparameter tunings and more data. Since our main point is not in getting good result but in implementing models in a consistent way, I'll leave the process of getting good results to readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_similarity(target, model, corpus):\n",
    "    print(\"TARGET SENTENCE: {}\".format(corpus[target]))\n",
    "    target_V = model.embedding_for(target)\n",
    "    \n",
    "    similarities = []\n",
    "    for doc in range(model.corpus_length):\n",
    "        if doc == target: continue\n",
    "        \n",
    "        vector = model.embedding_for(doc)\n",
    "        cosine_sim = 1 - spatial.distance.cosine(target_V, vector)\n",
    "        similarities.append([corpus[doc].strip(), cosine_sim])\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET SENTENCE: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\",\n",
       "  0.5692881345748901],\n",
       " ['This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!',\n",
       "  0.5344139337539673],\n",
       " ['I am very satisfied with my Twizzler purchase.  I shared these with others and we have all enjoyed them.  I will definitely be ordering more.',\n",
       "  0.528869092464447],\n",
       " [\"Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\",\n",
       "  0.5215874314308167],\n",
       " ['The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I.',\n",
       "  0.515813410282135]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_similarity(0, doc2vec, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec.generate_tsne('log/doc2vec/fig.png', size=(30,20), doc_count=50, corpus=corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
