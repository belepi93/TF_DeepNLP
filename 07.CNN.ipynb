{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CNN - Convolutional Neural Networks for Sentence Classification\n",
    "Yoon Kim's Paper introduced a very simple and lightweight CNN architecture for sentence classification. If you use a pretrained embedding here, The number of model's parameter will be lower than 1mil but it's still powerful! In this notebook, you'll be able to train the model for several datasets.\n",
    "\n",
    "### References\n",
    "- [Convolutional Neural Networks for Sentence Classification - Kim, 2014](https://arxiv.org/abs/1408.5882)\n",
    "- [A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional\n",
    "Neural Networks for Sentence Classification - Zhang et al. 2015](https://arxiv.org/pdf/1510.03820.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 12](http://web.stanford.edu/class/cs224n/lectures/lecture12.pdf)\n",
    "- [yoonkim/CNN_sentence](https://github.com/yoonkim/CNN_sentence)\n",
    "- [harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Preprocessing codes and processed data are borrowed from [harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch).\n",
    "\n",
    "It's getting harder and harder to preprecess data in our model class. So we will preprocess before using `fit_to_corpus()` method as far as we can.\n",
    "\n",
    "You have to select among these datasets `MR/SST1/SST2/Subj/TREC/CR/MPQA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.sentiment_datasets.preprocess as preprocess\n",
    "from models import CNN\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB.pkl exists! loading from pkl..\n"
     ]
    }
   ],
   "source": [
    "w2v, train, train_label, test, test_label, dev, dev_label, word_to_idx = preprocess.build_dataset(\"IMDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev, train_label, test_label, dev_label = \\\n",
    "    preprocess.train_test_dev_split(train, test, dev, train_label, test_label, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [train, train_label, dev, dev_label, w2v, word_to_idx]\n",
    "test_data = [test, test_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04180000\n"
     ]
    }
   ],
   "source": [
    "model = CNN.CNN(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_corpus(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 361502\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch training time: 96.66446566581726\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 0.52184386, train_accruacy = 0.72708240\n",
      "valid_loss = 0.31752651, valid_accuracy = 0.86480000\n",
      "\n",
      "Epoch training time: 96.73893523216248\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 0.31421108, train_accruacy = 0.86677060\n",
      "valid_loss = 0.27972316, valid_accuracy = 0.88519999\n",
      "\n",
      "Epoch training time: 96.66560077667236\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 0.25305322, train_accruacy = 0.89679287\n",
      "valid_loss = 0.27324377, valid_accuracy = 0.88559999\n",
      "\n",
      "Epoch training time: 97.23969650268555\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 0.20792951, train_accruacy = 0.91790646\n",
      "valid_loss = 0.24807674, valid_accuracy = 0.90280000\n",
      "\n",
      "Epoch training time: 97.29168891906738\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 0.17089777, train_accruacy = 0.94048998\n",
      "valid_loss = 0.24312144, valid_accuracy = 0.90240000\n",
      "\n",
      "Epoch training time: 96.97170567512512\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 0.14328896, train_accruacy = 0.95207127\n",
      "valid_loss = 0.23618293, valid_accuracy = 0.90279999\n",
      "\n",
      "Epoch training time: 97.19346594810486\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 0.11929771, train_accruacy = 0.96338530\n",
      "valid_loss = 0.24520831, valid_accuracy = 0.89800000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(20, save_dir=\"save/07_cnn\", log_dir=\"log/07_cnn\", print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/07_cnn/epoch020_0.4196.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 361502\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 0.35618254, test accuracy = 0.85055555\n",
      "test samples: 001800, time elapsed: 0.0993, time per one batch: 0.0028\n"
     ]
    }
   ],
   "source": [
    "model.test(test_data, load_dir=\"save/07_cnn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
