{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CNN - Convolutional Neural Networks for Sentence Classification\n",
    "Yoon Kim's Paper introduced a very simple and lightweight CNN architecture for sentence classification. If you use a pretrained embedding here, The number of model's parameter will be lower than 1mil but it's still powerful! In this notebook, you'll be able to train the model for several datasets.\n",
    "\n",
    "### References\n",
    "- [A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional\n",
    "Neural Networks for Sentence Classification - Zhang et al. 2015](https://arxiv.org/pdf/1510.03820.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 12](http://web.stanford.edu/class/cs224n/lectures/lecture12.pdf)\n",
    "- [yoonkim/CNN_sentence](https://github.com/yoonkim/CNN_sentence)\n",
    "- [harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Preprocessing codes are borrowed from [harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch).\n",
    "\n",
    "It's getting harder and harder to preprecess data in our model class. So we will preprocess before using `fit_to_corpus()` method as far as we can.\n",
    "\n",
    "You have to select among these datasets `MR/SST1/SST2/Subj/TREC/CR/MPQA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.sentiment_datasets.preprocess as preprocess\n",
    "from models import CNN\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST2.pkl exists! loading from pkl..\n"
     ]
    }
   ],
   "source": [
    "w2v, train, train_label, test, test_label, dev, dev_label, word_to_idx = preprocess.build_dataset(\"SST2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_dev_split(train, test, dev, train_label, test_label, dev_label):\n",
    "    cnt = 0\n",
    "    for data in [train, test, dev]:\n",
    "        if len(data) != 0:\n",
    "            cnt += 1\n",
    "    \n",
    "    if cnt == 0:\n",
    "        raise ValueError(\"not a proper train,dev,test input\")\n",
    "        \n",
    "    elif cnt == 1:  # only train set is provided.\n",
    "        train_set = list(zip(train, train_label))\n",
    "        random.shuffle(train_set)\n",
    "        idx1 = int(len(train_set) * 0.8)\n",
    "        idx2 = int(len(train_set) * 0.9)\n",
    "        train_set, test_set, dev_set = train_set[:idx1], train_set[idx1:idx2], train_set[idx2:]\n",
    "        train, train_label = list(zip(*train_set))\n",
    "        test, test_label = list(zip(*test_set))\n",
    "        dev, dev_label = list(zip(*dev_set))\n",
    "\n",
    "    elif cnt == 2:  # train/test sets are provided.\n",
    "        train_set = list(zip(train, train_label))\n",
    "        random.shuffle(train_set)\n",
    "        idx1 = int(len(train_set) * 0.9)\n",
    "        train_set, dev_set = train_set[:idx1], train_set[idx1:]\n",
    "        train, train_label = list(zip(*train_set))\n",
    "        dev, dev_label = list(zip(*dev_set))\n",
    "\n",
    "    elif cnt == 3:  # train/test/dev sets are provided.\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Is it possible to reach here??\")\n",
    "        \n",
    "    return np.array(train), np.array(test), np.array(dev), \\\n",
    "           np.array(train_label), np.array(test_label), np.array(dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, dev, train_label, test_label, dev_label = \\\n",
    "    train_test_dev_split(train, test, dev, train_label, test_label, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [train, train_label, dev, dev_label, w2v, word_to_idx]\n",
    "test_data = [test, test_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04180000\n"
     ]
    }
   ],
   "source": [
    "model = CNN.CNN(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_corpus(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 361502\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch training time: 1.3180923461914062\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 0.60351326, train_accruacy = 0.66652174\n",
      "valid_loss = 0.46909564, valid_accuracy = 0.81411764\n",
      "\n",
      "Epoch training time: 0.8415021896362305\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 0.42120360, train_accruacy = 0.81043478\n",
      "valid_loss = 0.41845854, valid_accuracy = 0.81764705\n",
      "\n",
      "Epoch training time: 0.8786766529083252\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 0.33527417, train_accruacy = 0.86043478\n",
      "valid_loss = 0.42911296, valid_accuracy = 0.81529411\n",
      "\n",
      "Epoch training time: 0.8922944068908691\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 0.27726179, train_accruacy = 0.89362318\n",
      "valid_loss = 0.39775787, valid_accuracy = 0.83529412\n",
      "\n",
      "Epoch training time: 0.897108793258667\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 0.22240816, train_accruacy = 0.92492753\n",
      "valid_loss = 0.39718972, valid_accuracy = 0.83176469\n",
      "\n",
      "Epoch training time: 0.9152348041534424\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 0.18419420, train_accruacy = 0.94855073\n",
      "valid_loss = 0.39144568, valid_accuracy = 0.84235294\n",
      "\n",
      "Epoch training time: 0.8993251323699951\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 0.15265174, train_accruacy = 0.96217391\n",
      "valid_loss = 0.38765859, valid_accuracy = 0.82941176\n",
      "\n",
      "Epoch training time: 0.8932950496673584\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 0.12651254, train_accruacy = 0.97666667\n",
      "valid_loss = 0.39176854, valid_accuracy = 0.82352940\n",
      "\n",
      "Epoch training time: 0.9134259223937988\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 0.11027824, train_accruacy = 0.98028986\n",
      "valid_loss = 0.40197470, valid_accuracy = 0.83294117\n",
      "\n",
      "Epoch training time: 0.8879961967468262\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 0.09447449, train_accruacy = 0.98913044\n",
      "valid_loss = 0.40468481, valid_accuracy = 0.83647059\n",
      "\n",
      "Epoch training time: 0.8600740432739258\n",
      "\n",
      "Finished Epoch 11\n",
      "train_loss = 0.08282732, train_accruacy = 0.99086957\n",
      "valid_loss = 0.40206069, valid_accuracy = 0.83764706\n",
      "\n",
      "Epoch training time: 0.9056544303894043\n",
      "\n",
      "Finished Epoch 12\n",
      "train_loss = 0.07567611, train_accruacy = 0.99173913\n",
      "valid_loss = 0.39823523, valid_accuracy = 0.83529411\n",
      "\n",
      "Epoch training time: 0.8970088958740234\n",
      "\n",
      "Finished Epoch 13\n",
      "train_loss = 0.06738154, train_accruacy = 0.99347826\n",
      "valid_loss = 0.40687717, valid_accuracy = 0.83999999\n",
      "\n",
      "Epoch training time: 0.8926489353179932\n",
      "\n",
      "Finished Epoch 14\n",
      "train_loss = 0.06128198, train_accruacy = 0.99579710\n",
      "valid_loss = 0.40824153, valid_accuracy = 0.82941176\n",
      "\n",
      "Epoch training time: 0.8794896602630615\n",
      "\n",
      "Finished Epoch 15\n",
      "train_loss = 0.05552808, train_accruacy = 0.99739131\n",
      "valid_loss = 0.40381766, valid_accuracy = 0.83647058\n",
      "\n",
      "Epoch training time: 0.9399416446685791\n",
      "\n",
      "Finished Epoch 16\n",
      "train_loss = 0.05219107, train_accruacy = 0.99710145\n",
      "valid_loss = 0.41912156, valid_accuracy = 0.83294117\n",
      "\n",
      "Epoch training time: 0.8922019004821777\n",
      "\n",
      "Finished Epoch 17\n",
      "train_loss = 0.04670666, train_accruacy = 0.99797102\n",
      "valid_loss = 0.40895160, valid_accuracy = 0.83764705\n",
      "\n",
      "Epoch training time: 0.9626636505126953\n",
      "\n",
      "Finished Epoch 18\n",
      "train_loss = 0.04455842, train_accruacy = 0.99840580\n",
      "valid_loss = 0.42355653, valid_accuracy = 0.83764705\n",
      "\n",
      "Epoch training time: 0.9059262275695801\n",
      "\n",
      "Finished Epoch 19\n",
      "train_loss = 0.04085672, train_accruacy = 0.99855073\n",
      "valid_loss = 0.42067591, valid_accuracy = 0.83529411\n",
      "\n",
      "Epoch training time: 0.9679563045501709\n",
      "\n",
      "Finished Epoch 20\n",
      "train_loss = 0.03798085, train_accruacy = 0.99826087\n",
      "valid_loss = 0.41955179, valid_accuracy = 0.83294117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(20, save_dir=\"save/07_cnn\", log_dir=\"log/07_cnn\", print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/07_cnn/epoch020_0.4196.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 361502\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 0.35618254, test accuracy = 0.85055555\n",
      "test samples: 001800, time elapsed: 0.0993, time per one batch: 0.0028\n"
     ]
    }
   ],
   "source": [
    "model.test(test_data, load_dir=\"save/07_cnn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
