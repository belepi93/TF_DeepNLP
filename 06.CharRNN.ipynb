{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CharRNN\n",
    "\n",
    "Andrej Karpathy's famous [char-rnn](https://github.com/karpathy/char-rnn) is already implemented in TensorFlow by [sherjilozair](https://github.com/sherjilozair/char-rnn-tensorflow). I borrowed their codes and made some tweaks in order to make a char-rnn model with consistent interface with my other models. In fact, i think their codes are a lot more easier to see and have a lot of things to learn. I'd recommend you look at these repositories!\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 8](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 9](http://web.stanford.edu/class/cs224n/lectures/lecture9.pdf)\n",
    "- [mkroutikov/tf-lstm-char-cnn](https://github.com/mkroutikov/tf-lstm-char-cnn)\n",
    "- [sherjilozair/char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow)\n",
    "- [karpathy/char-rnn](https://github.com/karpathy/char-rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CharRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(data_dir):\n",
    "    corpus = []\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            corpus.append(line)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus(\"data/rnn/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04160104\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN.CharRNN(hidden_size=512,\n",
    "                        cell=\"LSTM\",\n",
    "                        num_unroll_steps=30,\n",
    "                        learning_rate=0.01,\n",
    "                        batch_size=64,\n",
    "                        num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 3316289\n",
      "--------------------------------------------------------------------------------\n",
      "000200: 1 [00200/00580], train_loss/perplexity = 3.20055103/24.5460529 secs/batch = 0.0283\n",
      "000400: 1 [00400/00580], train_loss/perplexity = 2.30833673/10.0576820 secs/batch = 0.0285\n",
      "Epoch training time: 16.374430656433105\n",
      "Finished Epoch 1\n",
      "train_loss = 2.74678957, perplexity = 15.59249286\n",
      "000780: 2 [00200/00580], train_loss/perplexity = 1.87262344/6.5053406 secs/batch = 0.0276\n",
      "000980: 2 [00400/00580], train_loss/perplexity = 1.74065685/5.7010870 secs/batch = 0.0276\n",
      "Epoch training time: 16.24542999267578\n",
      "Finished Epoch 2\n",
      "train_loss = 1.83759608, perplexity = 6.28142008\n",
      "001360: 3 [00200/00580], train_loss/perplexity = 1.68111587/5.3715467 secs/batch = 0.0278\n",
      "001560: 3 [00400/00580], train_loss/perplexity = 1.59330547/4.9199848 secs/batch = 0.0281\n",
      "Epoch training time: 16.230884790420532\n",
      "Finished Epoch 3\n",
      "train_loss = 1.65265310, perplexity = 5.22081282\n",
      "001940: 4 [00200/00580], train_loss/perplexity = 1.56416130/4.7786655 secs/batch = 0.0280\n",
      "002140: 4 [00400/00580], train_loss/perplexity = 1.55122161/4.7172294 secs/batch = 0.0286\n",
      "Epoch training time: 16.270426750183105\n",
      "Finished Epoch 4\n",
      "train_loss = 1.57177157, perplexity = 4.81517105\n",
      "002520: 5 [00200/00580], train_loss/perplexity = 1.50944221/4.5242066 secs/batch = 0.0284\n",
      "002720: 5 [00400/00580], train_loss/perplexity = 1.47510386/4.3714895 secs/batch = 0.0279\n",
      "Epoch training time: 16.26349687576294\n",
      "Finished Epoch 5\n",
      "train_loss = 1.52350498, perplexity = 4.58827886\n",
      "003100: 6 [00200/00580], train_loss/perplexity = 1.46371675/4.3219934 secs/batch = 0.0277\n",
      "003300: 6 [00400/00580], train_loss/perplexity = 1.46557271/4.3300223 secs/batch = 0.0281\n",
      "Epoch training time: 16.297768592834473\n",
      "Finished Epoch 6\n",
      "train_loss = 1.49130130, perplexity = 4.44287329\n",
      "003680: 7 [00200/00580], train_loss/perplexity = 1.44234204/4.2305923 secs/batch = 0.0280\n",
      "003880: 7 [00400/00580], train_loss/perplexity = 1.45955086/4.3040261 secs/batch = 0.0286\n",
      "Epoch training time: 16.290769815444946\n",
      "Finished Epoch 7\n",
      "train_loss = 1.46935684, perplexity = 4.34643880\n",
      "004260: 8 [00200/00580], train_loss/perplexity = 1.42548132/4.1598597 secs/batch = 0.0272\n",
      "004460: 8 [00400/00580], train_loss/perplexity = 1.43851233/4.2144213 secs/batch = 0.0281\n",
      "Epoch training time: 16.264429330825806\n",
      "Finished Epoch 8\n",
      "train_loss = 1.44918810, perplexity = 4.25965469\n",
      "004840: 9 [00200/00580], train_loss/perplexity = 1.41358864/4.1106806 secs/batch = 0.0284\n",
      "005040: 9 [00400/00580], train_loss/perplexity = 1.40604770/4.0797987 secs/batch = 0.0281\n",
      "Epoch training time: 16.319333791732788\n",
      "Finished Epoch 9\n",
      "train_loss = 1.43501665, perplexity = 4.19971493\n",
      "005420: 10 [00200/00580], train_loss/perplexity = 1.38664412/4.0013995 secs/batch = 0.0283\n",
      "005620: 10 [00400/00580], train_loss/perplexity = 1.40390205/4.0710545 secs/batch = 0.0286\n",
      "Epoch training time: 16.303946495056152\n",
      "Finished Epoch 10\n",
      "train_loss = 1.42306946, perplexity = 4.14983869\n",
      "006000: 11 [00200/00580], train_loss/perplexity = 1.36003745/3.8963392 secs/batch = 0.0276\n",
      "006200: 11 [00400/00580], train_loss/perplexity = 1.38267255/3.9855390 secs/batch = 0.0269\n",
      "Epoch training time: 16.28044605255127\n",
      "Finished Epoch 11\n",
      "train_loss = 1.41090060, perplexity = 4.09964589\n",
      "006580: 12 [00200/00580], train_loss/perplexity = 1.40665472/4.0822763 secs/batch = 0.0280\n",
      "006780: 12 [00400/00580], train_loss/perplexity = 1.37418246/3.9518447 secs/batch = 0.0282\n",
      "Epoch training time: 16.281493186950684\n",
      "Finished Epoch 12\n",
      "train_loss = 1.40179152, perplexity = 4.06247147\n",
      "007160: 13 [00200/00580], train_loss/perplexity = 1.37911916/3.9714019 secs/batch = 0.0277\n",
      "007360: 13 [00400/00580], train_loss/perplexity = 1.36508679/3.9160628 secs/batch = 0.0281\n",
      "Epoch training time: 16.292144536972046\n",
      "Finished Epoch 13\n",
      "train_loss = 1.39600602, perplexity = 4.03903586\n",
      "007740: 14 [00200/00580], train_loss/perplexity = 1.37647593/3.9609184 secs/batch = 0.0272\n",
      "007940: 14 [00400/00580], train_loss/perplexity = 1.34298110/3.8304455 secs/batch = 0.0278\n",
      "Epoch training time: 16.287570238113403\n",
      "Finished Epoch 14\n",
      "train_loss = 1.38513145, perplexity = 3.99535107\n",
      "008320: 15 [00200/00580], train_loss/perplexity = 1.33474350/3.7990215 secs/batch = 0.0278\n",
      "008520: 15 [00400/00580], train_loss/perplexity = 1.35206461/3.8653979 secs/batch = 0.0281\n",
      "Epoch training time: 16.279181003570557\n",
      "Finished Epoch 15\n",
      "train_loss = 1.37861448, perplexity = 3.96939814\n",
      "008900: 16 [00200/00580], train_loss/perplexity = 1.32533431/3.7634432 secs/batch = 0.0278\n",
      "009100: 16 [00400/00580], train_loss/perplexity = 1.35105503/3.8614974 secs/batch = 0.0281\n",
      "Epoch training time: 16.319833278656006\n",
      "Finished Epoch 16\n",
      "train_loss = 1.37353149, perplexity = 3.94927292\n",
      "009480: 17 [00200/00580], train_loss/perplexity = 1.31500006/3.7247512 secs/batch = 0.0284\n",
      "009680: 17 [00400/00580], train_loss/perplexity = 1.33173347/3.7876034 secs/batch = 0.0285\n",
      "Epoch training time: 16.285686254501343\n",
      "Finished Epoch 17\n",
      "train_loss = 1.36605201, perplexity = 3.91984459\n",
      "010060: 18 [00200/00580], train_loss/perplexity = 1.31473327/3.7237575 secs/batch = 0.0287\n",
      "010260: 18 [00400/00580], train_loss/perplexity = 1.34968853/3.8562243 secs/batch = 0.0287\n",
      "Epoch training time: 16.324463367462158\n",
      "Finished Epoch 18\n",
      "train_loss = 1.35887650, perplexity = 3.89181840\n",
      "010640: 19 [00200/00580], train_loss/perplexity = 1.31081843/3.7092083 secs/batch = 0.0278\n",
      "010840: 19 [00400/00580], train_loss/perplexity = 1.31647158/3.7302363 secs/batch = 0.0276\n",
      "Epoch training time: 16.31505298614502\n",
      "Finished Epoch 19\n",
      "train_loss = 1.34932145, perplexity = 3.85480897\n",
      "011220: 20 [00200/00580], train_loss/perplexity = 1.29285598/3.6431766 secs/batch = 0.0285\n",
      "011420: 20 [00400/00580], train_loss/perplexity = 1.32753730/3.7717433 secs/batch = 0.0284\n",
      "Epoch training time: 16.287211418151855\n",
      "Finished Epoch 20\n",
      "train_loss = 1.34510513, perplexity = 3.83859007\n"
     ]
    }
   ],
   "source": [
    "model.train(20, save_dir=\"save/06_char_rnn\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/06_char_rnn/epoch020_1.3451.model\n",
      "KING RICHARD II:\n",
      "I have no loss and be more so tales, sir? to my word,\n",
      "Tell my heart will took them this and shall to hell;\n",
      "If thou art some firm and shall she hear thee with\n",
      "her that should thou art too, and shall with such a too mercy,\n",
      "there is a second and that hour would\n",
      "I have as harm which thou didst saw well see too lunt to me?\n",
      "\n",
      "PETER:\n",
      "Why, this, thou'rt no more wise at money,\n",
      "And thy harmour and the war to stay, which I will think\n",
      "A bleeding sour the church, and both that brange\n",
      "Trial then a words with any times; and this was.\n",
      "\n",
      "LUCENTIO:\n",
      "How?\n",
      "\n",
      "First Citizen:\n",
      "Thy woman shall branch the cheer, then he is,\n",
      "I say, an old with a seat were too so too much,\n",
      "And minumones, he may brand this tribunes,\n",
      "And tell me: the streaking, see, so was the word,\n",
      "Thought all the country, and were not he and seal to-day,\n",
      "I am bearing to her to take you to the husband, though a shelcase seemet the soul\n",
      "Was weak that shuse, this win thou death. Thy brother\n",
      "Who will I have him took an earth that be another.\n",
      "\n",
      "BIONDELLO:\n",
      "There shall have as him.\n",
      "\n",
      "First Gentleman:\n",
      "Hear you at his case to me to set attunn'd and help:\n",
      "And to the heart that thou hast train thy worth\n",
      "To business to her hearts: these thoughts,\n",
      "Without the high time on thy body's store,\n",
      "That then, thou dost before those woman and the subject town, which I may\n",
      "high her to so more. Thou dost never\n",
      "Are to stop more as the cap that show her brief and so subject,\n",
      "And so my soul hath wrought the sense of my battle.\n",
      "What would we have the sight to my lady bleeds.\n",
      "\n",
      "BUCKINGHAM:\n",
      "A mistress, having a thirt, and be as honest too.\n",
      "\n",
      "PETER:\n",
      "Why, what my honour is says we say, as I stay.\n",
      "\n",
      "BAPTISTA:\n",
      "Ay, a measure.\n",
      "\n",
      "POMPEY:\n",
      "Why, what it should show me.\n",
      "\n",
      "COMINIUS:\n",
      "As I could bring me too,\n",
      "And which he that body to talk all one wings it in man\n",
      "taken to make a soldier, some husband and a witter weeps in his heart,\n",
      "What a steeled wits of my lieges and my bleed and set\n",
      "I was that they shall say to the time to spend\n",
      "Withal, being bring hither therein to h\n"
     ]
    }
   ],
   "source": [
    "print(model.sample(2000, load_dir=\"save/06_char_rnn\", starter_seq=\"KING \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
