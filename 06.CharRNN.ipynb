{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CharRNN\n",
    "\n",
    "Andrej Karpathy's famous [char-rnn](https://github.com/karpathy/char-rnn) is already implemented in TensorFlow by [sherjilozair](https://github.com/sherjilozair/char-rnn-tensorflow). I borrowed their codes and made some tweaks in order to make a char-rnn model with consistent interface with my other models. In fact, i think their codes are a lot more easier to see and have a lot of things to learn. I'd recommend you look at these repositories!\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 8](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 9](http://web.stanford.edu/class/cs224n/lectures/lecture9.pdf)\n",
    "- [mkroutikov/tf-lstm-char-cnn](https://github.com/mkroutikov/tf-lstm-char-cnn)\n",
    "- [sherjilozair/char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow)\n",
    "- [karpathy/char-rnn](https://github.com/karpathy/char-rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CharRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(data_dir):\n",
    "    corpus = []\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            corpus.append(line)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus(\"data/rnn/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04160104\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN.CharRNN(hidden_size=512,\n",
    "                        cell=\"LSTM\",\n",
    "                        num_unroll_steps=30,\n",
    "                        learning_rate=0.01,\n",
    "                        batch_size=64,\n",
    "                        num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_to_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 3316289\n",
      "--------------------------------------------------------------------------------\n",
      "000200: 1 [00200/00580], train_loss/perplexity = 3.32726717/27.8620949 secs/batch = 0.0280\n",
      "000400: 1 [00400/00580], train_loss/perplexity = 3.18615389/24.1951904 secs/batch = 0.0286\n",
      "Epoch training time: 17.009737730026245\n",
      "Finished Epoch 1\n",
      "train_loss = 3.33594661, perflexity = 28.10497503\n",
      "000780: 2 [00200/00580], train_loss/perplexity = 2.83127499/16.9670792 secs/batch = 0.0391\n",
      "000980: 2 [00400/00580], train_loss/perplexity = 2.52668476/12.5119572 secs/batch = 0.0273\n",
      "Epoch training time: 17.986651182174683\n",
      "Finished Epoch 2\n",
      "train_loss = 2.72014301, perflexity = 15.18249335\n",
      "001360: 3 [00200/00580], train_loss/perplexity = 2.21134901/9.1280222 secs/batch = 0.0281\n",
      "001560: 3 [00400/00580], train_loss/perplexity = 2.08865452/8.0740442 secs/batch = 0.0284\n",
      "Epoch training time: 16.29326343536377\n",
      "Finished Epoch 3\n",
      "train_loss = 2.11923080, perflexity = 8.32473163\n",
      "001940: 4 [00200/00580], train_loss/perplexity = 2.04758215/7.7491422 secs/batch = 0.0288\n",
      "002140: 4 [00400/00580], train_loss/perplexity = 1.94928586/7.0236697 secs/batch = 0.0283\n",
      "Epoch training time: 16.272318601608276\n",
      "Finished Epoch 4\n",
      "train_loss = 1.94077936, perflexity = 6.96417647\n",
      "002520: 5 [00200/00580], train_loss/perplexity = 1.99504268/7.3525167 secs/batch = 0.0287\n",
      "002720: 5 [00400/00580], train_loss/perplexity = 1.88049400/6.5567431 secs/batch = 0.0275\n",
      "Epoch training time: 16.462920904159546\n",
      "Finished Epoch 5\n",
      "train_loss = 1.85814505, perflexity = 6.41183213\n",
      "003100: 6 [00200/00580], train_loss/perplexity = 1.93413138/6.9180322 secs/batch = 0.0276\n",
      "003300: 6 [00400/00580], train_loss/perplexity = 1.81848359/6.1625066 secs/batch = 0.0277\n",
      "Epoch training time: 16.257143020629883\n",
      "Finished Epoch 6\n",
      "train_loss = 1.80758525, perflexity = 6.09571003\n",
      "003680: 7 [00200/00580], train_loss/perplexity = 1.86718428/6.4700527 secs/batch = 0.0267\n",
      "003880: 7 [00400/00580], train_loss/perplexity = 1.79424047/6.0149045 secs/batch = 0.0286\n",
      "Epoch training time: 16.245591402053833\n",
      "Finished Epoch 7\n",
      "train_loss = 1.77073339, perflexity = 5.87516055\n",
      "004260: 8 [00200/00580], train_loss/perplexity = 1.83695269/6.2773800 secs/batch = 0.0266\n",
      "004460: 8 [00400/00580], train_loss/perplexity = 1.77813768/5.9188232 secs/batch = 0.0279\n",
      "Epoch training time: 16.246111392974854\n",
      "Finished Epoch 8\n",
      "train_loss = 1.74334977, perflexity = 5.71646020\n",
      "004840: 9 [00200/00580], train_loss/perplexity = 1.78770363/5.9757142 secs/batch = 0.0279\n",
      "005040: 9 [00400/00580], train_loss/perplexity = 1.74734139/5.7393236 secs/batch = 0.0283\n",
      "Epoch training time: 16.445680379867554\n",
      "Finished Epoch 9\n",
      "train_loss = 1.72164467, perflexity = 5.59372074\n",
      "005420: 10 [00200/00580], train_loss/perplexity = 1.80376410/6.0724621 secs/batch = 0.0276\n",
      "005620: 10 [00400/00580], train_loss/perplexity = 1.73246825/5.6545935 secs/batch = 0.0279\n",
      "Epoch training time: 16.29682755470276\n",
      "Finished Epoch 10\n",
      "train_loss = 1.70489917, perflexity = 5.50083098\n"
     ]
    }
   ],
   "source": [
    "model.train(10, save_dir=\"save/06_char_rnn\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/06_char_rnn/epoch010_1.7049.model\n",
      "The mine is make the son of the most banith them.\n",
      "\n",
      "PROSPERO:\n",
      "What what, and he bite to shall braving,\n",
      "The storis more at the son\n",
      "Than the mine and\n",
      "As\n",
      "assest the wit this thank me to be all.\n",
      "\n",
      "PETRUCHIO:\n",
      "And should she broke off thou the match of the marching on my souls with as tank another that have sir; soul touchest and servant\n",
      "To beso the sir sight with madam the daughter of me were that all the heart think'd with a mitters:\n",
      "I saids\n",
      "And sights our dream mine orter out\n",
      "Which as\n",
      "This stain\n",
      "I prison.\n",
      "\n",
      "ARIEL:\n",
      "O mine the honour\n",
      "As she havish most borness.\n",
      "\n",
      "GRUMIO:\n",
      "With mount at my daughter.\n",
      "\n",
      "PROSPERO:\n",
      "Truth, as he save\n",
      "With me which thou make thee,\n",
      "A drather steer, to this ster my dreamen a promore as when thou hopes my hast ships as with his thee\n",
      "Is think\n",
      "Thou here but short\n",
      "As the dear my merrant\n",
      "Thou womined my mark as him.\n",
      "\n",
      "ANTONIO:\n",
      "Thou hast men morrow he be thy hing thousand, wings of his missis midity,\n",
      "And mean horse.\n",
      "\n",
      "BAPTISTA:\n",
      "That they brain our distorther to her.\n",
      "\n",
      "PROSPERO:\n",
      "I do him.\n",
      "\n",
      "Lord, so,\n",
      "The watch women and hast means there water way that she wonse that shall needs\n",
      "Twithou teach made the\n",
      "musiditaness too state, all the wead me.\n",
      "\n",
      "PROSPERO:\n",
      "O that still this tistith\n",
      "Were straight thon the side,\n",
      "She drister, burnt him out mit thing he came bid this worlds.\n",
      "\n",
      "ARIAN:\n",
      "What hith me out and morrow\n",
      "Where is show thy compan an angry that the hurbouth of myself or horses my case to bush the sir, bour, the day my hatters that shall had honour'd of the worth that steed\n",
      "Taid their shall sour\n",
      "Our depasse thine hate the hope wath her thousand where shall stir the saiders when my mornant to be soul what straight\n",
      "The saint that a properses thing,\n",
      "And he did but ming that the wiss make her so that wathan more thou shapt signity\n",
      "Of the his\n",
      "That this was, when hine that, to the heath harm that women of him,\n",
      "If I had no bosom\n",
      "I pridowers of tread a prophidance\n",
      "There is most staid\n",
      "And so band\n",
      "Than to my saited we have thy man so time,--\n",
      "\n",
      "BAPTISTA:\n",
      "What, who make all, and to brea\n"
     ]
    }
   ],
   "source": [
    "print(model.sample(2000, load_dir=\"save/06_char_rnn\", starter_seq=\"The \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Result!\n",
    "If we train more epochs, Maybe we could get a much more nicer result than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
