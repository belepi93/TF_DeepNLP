{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word2Vec\n",
    "\n",
    "### References\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality - Mikolov et al. 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [TensorFlow Tutorial - Vector Representations of Words](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Word2Vec\n",
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Methods\n",
    "Define utility methods for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(texts, stops):\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    # remove empty strings\n",
    "    texts = [x for x in texts if x != '']\n",
    "    \n",
    "    return(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Corpus\n",
    "We will use [Amazon Food Review Corpus](https://www.kaggle.com/snap/amazon-fine-food-reviews). If we have already normalized and saved corpus with pickle, load the data. If not, load raw data and normalize the corpus and save it. This will takes some time since these codes are not optimized for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"data/amazon_food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found amazon_corpus.pkl. Loading Corpus..\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(DATA_DIR,\"amazon_corpus.pkl\")):\n",
    "    print(\"Found amazon_corpus.pkl. Loading Corpus..\")\n",
    "    with open(os.path.join(DATA_DIR, \"amazon_corpus.pkl\"), \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print(\"No amazon_corpus.pkl. Preprocessing Corpus..\")\n",
    "    with open(os.path.join(DATA_DIR, \"Reviews.csv\"), 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)\n",
    "        corpus = [line[-1] for line in reader]\n",
    "\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    corpus = normalize_corpus(corpus, stops)\n",
    "\n",
    "    corpus = [line.split(' ') for line in corpus]\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, \"amazon_corpus.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387867"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten(corpus[:10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.Word2VecModel(150,5, subsampling_threshold=1e-6, max_vocab_size=100000, negative_sample_size=10,\n",
    "                                  learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "word2vec.fit_to_corpus(corpus[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TensorBoard summaries to log/word2vec\n",
      "Saving TensorFlow models to save/word2vec\n",
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 7092162\n",
      "Total number of batches: 6990\n",
      "--------------------------------------------------------------------------------\n",
      "step: 1000, epoch:1, time/batch: 0.005406327724456787, avg_loss: 55.443824768066406\n",
      "step: 2000, epoch:1, time/batch: 0.004623481512069702, avg_loss: 39.67242431640625\n",
      "step: 3000, epoch:1, time/batch: 0.004372473478317261, avg_loss: 33.468414306640625\n",
      "step: 4000, epoch:1, time/batch: 0.004261737585067749, avg_loss: 28.45407485961914\n",
      "step: 5000, epoch:1, time/batch: 0.004471955060958863, avg_loss: 26.627120971679688\n",
      "Saved summaries at step 5000\n",
      "Saved a model at step 5000\n",
      "step: 6000, epoch:1, time/batch: 0.00408284330368042, avg_loss: 24.711214065551758\n",
      "step: 7000, epoch:2, time/batch: 0.003792802333831787, avg_loss: 21.486061096191406\n",
      "step: 8000, epoch:2, time/batch: 0.0042252447605133055, avg_loss: 20.896268844604492\n",
      "step: 9000, epoch:2, time/batch: 0.004437919616699219, avg_loss: 19.524084091186523\n",
      "step: 10000, epoch:2, time/batch: 0.004652540445327759, avg_loss: 18.967453002929688\n",
      "Saved summaries at step 10000\n",
      "Saved a model at step 10000\n",
      "step: 11000, epoch:2, time/batch: 0.003910586833953857, avg_loss: 18.423961639404297\n",
      "step: 12000, epoch:2, time/batch: 0.0037179675102233887, avg_loss: 17.321155548095703\n",
      "step: 13000, epoch:2, time/batch: 0.0043402349948883055, avg_loss: 16.984315872192383\n",
      "step: 14000, epoch:3, time/batch: 0.004421251773834229, avg_loss: 20.3210506439209\n",
      "step: 15000, epoch:3, time/batch: 0.004570011615753174, avg_loss: 16.12887954711914\n",
      "Saved summaries at step 15000\n",
      "Saved a model at step 15000\n",
      "step: 16000, epoch:3, time/batch: 0.0038284449577331544, avg_loss: 15.922073364257812\n",
      "step: 17000, epoch:3, time/batch: 0.003772907018661499, avg_loss: 15.489058494567871\n",
      "step: 18000, epoch:3, time/batch: 0.004280267000198365, avg_loss: 15.270845413208008\n",
      "step: 19000, epoch:3, time/batch: 0.004389530420303345, avg_loss: 15.83669376373291\n",
      "step: 20000, epoch:3, time/batch: 0.004628628015518188, avg_loss: 15.599236488342285\n",
      "Saved summaries at step 20000\n",
      "Saved a model at step 20000\n",
      "step: 21000, epoch:4, time/batch: 0.003976476669311523, avg_loss: 12.330085754394531\n",
      "step: 22000, epoch:4, time/batch: 0.0036692399978637694, avg_loss: 14.607906341552734\n",
      "step: 23000, epoch:4, time/batch: 0.00425772476196289, avg_loss: 13.59107494354248\n",
      "step: 24000, epoch:4, time/batch: 0.004744861602783203, avg_loss: 14.332212448120117\n",
      "step: 25000, epoch:4, time/batch: 0.004245384216308594, avg_loss: 14.119909286499023\n",
      "Saved summaries at step 25000\n",
      "Saved a model at step 25000\n",
      "step: 26000, epoch:4, time/batch: 0.003852626323699951, avg_loss: 14.351688385009766\n",
      "step: 27000, epoch:4, time/batch: 0.004270337104797363, avg_loss: 14.534605026245117\n",
      "step: 28000, epoch:5, time/batch: 0.004316588163375854, avg_loss: 13.747591018676758\n",
      "step: 29000, epoch:5, time/batch: 0.00431021499633789, avg_loss: 13.691161155700684\n",
      "step: 30000, epoch:5, time/batch: 0.004509952545166016, avg_loss: 13.734404563903809\n",
      "Saved summaries at step 30000\n",
      "Saved a model at step 30000\n",
      "step: 31000, epoch:5, time/batch: 0.003989266872406006, avg_loss: 13.50318717956543\n",
      "step: 32000, epoch:5, time/batch: 0.003898332357406616, avg_loss: 13.77730655670166\n",
      "step: 33000, epoch:5, time/batch: 0.004087520360946655, avg_loss: 13.650062561035156\n",
      "step: 34000, epoch:5, time/batch: 0.004575697898864746, avg_loss: 13.813196182250977\n",
      "step: 35000, epoch:6, time/batch: 0.003985152721405029, avg_loss: 12.57744312286377\n",
      "Saved summaries at step 35000\n",
      "Saved a model at step 35000\n",
      "step: 36000, epoch:6, time/batch: 0.00383904767036438, avg_loss: 13.613101959228516\n",
      "step: 37000, epoch:6, time/batch: 0.003702934503555298, avg_loss: 13.699667930603027\n",
      "step: 38000, epoch:6, time/batch: 0.004380323886871338, avg_loss: 13.228816032409668\n",
      "step: 39000, epoch:6, time/batch: 0.0047535295486450194, avg_loss: 13.7545166015625\n",
      "step: 40000, epoch:6, time/batch: 0.004607126712799072, avg_loss: 13.5620698928833\n",
      "Saved summaries at step 40000\n",
      "Saved a model at step 40000\n",
      "step: 41000, epoch:6, time/batch: 0.0038136494159698485, avg_loss: 13.941421508789062\n",
      "step: 42000, epoch:7, time/batch: 0.0036959176063537597, avg_loss: 13.588883399963379\n",
      "step: 43000, epoch:7, time/batch: 0.004283302783966064, avg_loss: 13.394782066345215\n",
      "step: 44000, epoch:7, time/batch: 0.004697753429412842, avg_loss: 12.77048110961914\n",
      "step: 45000, epoch:7, time/batch: 0.0047356667518615725, avg_loss: 13.230435371398926\n",
      "Saved summaries at step 45000\n",
      "Saved a model at step 45000\n",
      "step: 46000, epoch:7, time/batch: 0.0039377565383911135, avg_loss: 13.361968994140625\n",
      "step: 47000, epoch:7, time/batch: 0.0036611809730529785, avg_loss: 13.85753059387207\n",
      "step: 48000, epoch:7, time/batch: 0.003614647626876831, avg_loss: 13.946952819824219\n",
      "step: 49000, epoch:8, time/batch: 0.0036562421321868896, avg_loss: 13.275799751281738\n",
      "step: 50000, epoch:8, time/batch: 0.004686852216720581, avg_loss: 13.169793128967285\n",
      "Saved summaries at step 50000\n",
      "Saved a model at step 50000\n",
      "step: 51000, epoch:8, time/batch: 0.0039938905239105225, avg_loss: 13.107361793518066\n",
      "step: 52000, epoch:8, time/batch: 0.0036426897048950197, avg_loss: 12.626391410827637\n",
      "step: 53000, epoch:8, time/batch: 0.0035814549922943114, avg_loss: 13.016178131103516\n",
      "step: 54000, epoch:8, time/batch: 0.0036401271820068358, avg_loss: 13.304654121398926\n",
      "step: 55000, epoch:8, time/batch: 0.0040380022525787354, avg_loss: 13.439136505126953\n",
      "Saved summaries at step 55000\n",
      "Saved a model at step 55000\n",
      "step: 56000, epoch:9, time/batch: 0.004539035558700562, avg_loss: 12.814637184143066\n",
      "step: 57000, epoch:9, time/batch: 0.004554020643234253, avg_loss: 12.785120964050293\n",
      "step: 58000, epoch:9, time/batch: 0.003646733522415161, avg_loss: 13.3670015335083\n",
      "step: 59000, epoch:9, time/batch: 0.0036577200889587403, avg_loss: 12.258503913879395\n",
      "step: 60000, epoch:9, time/batch: 0.003581751585006714, avg_loss: 12.684968948364258\n",
      "Saved summaries at step 60000\n",
      "Saved a model at step 60000\n",
      "step: 61000, epoch:9, time/batch: 0.0038693170547485353, avg_loss: 13.293062210083008\n",
      "step: 62000, epoch:9, time/batch: 0.0035791208744049073, avg_loss: 13.057134628295898\n",
      "step: 63000, epoch:10, time/batch: 0.003590446472167969, avg_loss: 12.079319953918457\n",
      "step: 64000, epoch:10, time/batch: 0.0035816800594329836, avg_loss: 12.560349464416504\n",
      "step: 65000, epoch:10, time/batch: 0.003999834299087525, avg_loss: 12.632917404174805\n",
      "Saved summaries at step 65000\n",
      "Saved a model at step 65000\n",
      "step: 66000, epoch:10, time/batch: 0.003944292068481446, avg_loss: 13.149045944213867\n",
      "step: 67000, epoch:10, time/batch: 0.0035939462184906004, avg_loss: 12.432448387145996\n",
      "step: 68000, epoch:10, time/batch: 0.003594918489456177, avg_loss: 13.326329231262207\n",
      "step: 69000, epoch:10, time/batch: 0.003623014688491821, avg_loss: 13.373409271240234\n"
     ]
    }
   ],
   "source": [
    "word2vec.train(10, log_dir=\"log/01_word2vec\", save_dir=\"save/01_word2vec\", print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, model):\n",
    "    target_V = model.embedding_for(target)\n",
    "    \n",
    "    similarities = []\n",
    "    for word in model.words:\n",
    "        if word == target: continue\n",
    "        \n",
    "        vector = model.embedding_for(word)\n",
    "        cosine_sim = 1 - spatial.distance.cosine(target_V, vector)\n",
    "        similarities.append([word, cosine_sim])\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['good', 0.7600875496864319],\n",
       " ['really', 0.7369396686553955],\n",
       " ['taste', 0.7281869053840637],\n",
       " ['flavor', 0.7144145369529724],\n",
       " ['br', 0.6923125386238098],\n",
       " ['recommend', 0.6900876760482788],\n",
       " ['would', 0.6897999048233032],\n",
       " ['product', 0.6886175870895386],\n",
       " ['one', 0.6810516119003296],\n",
       " ['drink', 0.6701462268829346]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity('great', word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.generate_tsne('log/01_word2vec/fig.png', size=(15,15), word_count=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
