{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. RNN Language Model\n",
    "\n",
    "We will construct a Language Model with Recurrent Neural Networks. In this notebook, we will only cover one-way RNN/GRU/LSTM language model but it will be possible to expand one way network with one layer to bi-directional deep recurrent neural networks. Also, it is also possible to use CNN with RNN in order to construct a language model. We will cover it in a near future.\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 8](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 9](http://web.stanford.edu/class/cs224n/lectures/lecture9.pdf)\n",
    "- [mkroutikov/tf-lstm-char-cnn](https://github.com/mkroutikov/tf-lstm-char-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RNN_LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(data_dir):\n",
    "    corpus = []\n",
    "    with open(data_dir, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            tmp_line = line.strip().split(' ')\n",
    "            corpus.append(tmp_line)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = read_corpus(\"data/ptb/ptb.train.txt\")\n",
    "valid_corpus = read_corpus(\"data/ptb/ptb.valid.txt\")\n",
    "test_corpus = read_corpus(\"data/ptb/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you modify some parts of RNN_LM.py, you will be able to use pretrained word embeddings for this model and compare the performance of the cases when you use pretrained embeddings or not. I'll use pretrained embedding in future models, but in this model i'll leave it to readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04151140\n"
     ]
    }
   ],
   "source": [
    "model = RNN_LM.RNN_LM(word_embedding_size=128,\n",
    "                      hidden_size=1024,\n",
    "                      cell=\"LSTM\",\n",
    "                      num_unroll_steps=30,\n",
    "                      learning_rate=0.01,\n",
    "                      batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_corpus(train_corpus, valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 16252688\n",
      "--------------------------------------------------------------------------------\n",
      "000200: 1 [00200/00485], train_loss/perplexity = 5.72356892/305.9950562 secs/batch = 0.0551\n",
      "000400: 1 [00400/00485], train_loss/perplexity = 5.62530708/277.3574524 secs/batch = 0.0544\n",
      "Epoch training time: 26.43303632736206\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 5.93656246, perflexity = 378.63113111\n",
      "validation_loss = 5.26249652, perflexity = 192.96262475\n",
      "\n",
      "000685: 2 [00200/00485], train_loss/perplexity = 5.67664146/291.9671936 secs/batch = 0.0516\n",
      "000885: 2 [00400/00485], train_loss/perplexity = 5.14571953/171.6949768 secs/batch = 0.0508\n",
      "Epoch training time: 26.540902614593506\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 5.16783772, perflexity = 175.53487074\n",
      "validation_loss = 5.02809705, perflexity = 152.64226618\n",
      "\n",
      "001170: 3 [00200/00485], train_loss/perplexity = 5.22686768/186.2086182 secs/batch = 0.0618\n",
      "001370: 3 [00400/00485], train_loss/perplexity = 4.98565435/146.2992706 secs/batch = 0.0504\n",
      "Epoch training time: 26.145248651504517\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 4.84875708, perflexity = 127.58171743\n",
      "validation_loss = 4.95610663, perflexity = 142.03970546\n",
      "\n",
      "001655: 4 [00200/00485], train_loss/perplexity = 5.09205484/162.7238922 secs/batch = 0.0524\n",
      "001855: 4 [00400/00485], train_loss/perplexity = 4.30914783/74.3770828 secs/batch = 0.0517\n",
      "Epoch training time: 25.323181629180908\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 4.62798013, perflexity = 102.30720760\n",
      "validation_loss = 4.95490736, perflexity = 141.86946290\n",
      "\n",
      "002140: 5 [00200/00485], train_loss/perplexity = 4.69710350/109.6291733 secs/batch = 0.0542\n",
      "002340: 5 [00400/00485], train_loss/perplexity = 4.33824444/76.5729904 secs/batch = 0.0495\n",
      "Epoch training time: 25.154919624328613\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 4.46695181, perflexity = 87.09084851\n",
      "validation_loss = 4.96883247, perflexity = 143.85882983\n",
      "\n",
      "002625: 6 [00200/00485], train_loss/perplexity = 4.59208870/98.7003708 secs/batch = 0.0514\n",
      "002825: 6 [00400/00485], train_loss/perplexity = 4.50470209/90.4413986 secs/batch = 0.0526\n",
      "Epoch training time: 25.25392508506775\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 4.33330471, perflexity = 76.19567533\n",
      "validation_loss = 5.00772078, perflexity = 149.56345941\n",
      "\n",
      "003110: 7 [00200/00485], train_loss/perplexity = 3.32034922/27.6700115 secs/batch = 0.0508\n",
      "003310: 7 [00400/00485], train_loss/perplexity = 4.64129353/103.6783752 secs/batch = 0.0519\n",
      "Epoch training time: 25.325862169265747\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 4.21998213, perflexity = 68.03226837\n",
      "validation_loss = 5.05769993, perflexity = 157.22846324\n",
      "\n",
      "003595: 8 [00200/00485], train_loss/perplexity = 3.86827517/47.8597641 secs/batch = 0.0509\n",
      "003795: 8 [00400/00485], train_loss/perplexity = 3.28636217/26.7453918 secs/batch = 0.0516\n",
      "Epoch training time: 25.410638093948364\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 4.12330123, perflexity = 61.76279916\n",
      "validation_loss = 5.10862461, perflexity = 165.44265022\n",
      "\n",
      "004080: 9 [00200/00485], train_loss/perplexity = 3.94176865/51.5096245 secs/batch = 0.0510\n",
      "004280: 9 [00400/00485], train_loss/perplexity = 4.44026279/84.7972260 secs/batch = 0.0512\n",
      "Epoch training time: 25.329052686691284\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 4.04214579, perflexity = 56.94841092\n",
      "validation_loss = 5.17175220, perflexity = 176.22334613\n",
      "\n",
      "004565: 10 [00200/00485], train_loss/perplexity = 3.90211964/49.5072746 secs/batch = 0.0518\n",
      "004765: 10 [00400/00485], train_loss/perplexity = 4.31805229/75.0423279 secs/batch = 0.0532\n",
      "Epoch training time: 25.370490789413452\n",
      "\n",
      "Evaluating..\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 3.97872371, perflexity = 53.44877439\n",
      "validation_loss = 5.23839033, perflexity = 188.36665048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(10, save_dir=\"save/05_rnn_lm\", print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/05_rnn_lm/epoch010_5.2384.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 16252688\n",
      "--------------------------------------------------------------------------------\n",
      "test loss = 5.09814984, perplexity = 163.71872125\n",
      "test samples: 002747, time elapsed: 0.8746, time per one batch: 0.0203\n"
     ]
    }
   ],
   "source": [
    "model.test(test_corpus, load_dir=\"save/05_rnn_lm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
