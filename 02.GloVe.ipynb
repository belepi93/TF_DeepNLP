{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GloVe - Global Vectors for Word Representation\n",
    "\n",
    "### References\n",
    "- [GloVe: Global Vectors for Word Representation, Pennington et al. 2014](https://www.aclweb.org/anthology/D14-1162)\n",
    "- [GradySimon/tensorflow-glove](https://github.com/GradySimon/tensorflow-glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GloVe\n",
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Methods\n",
    "Define utility methods for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(texts, stops):\n",
    "    # Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # Remove stopwords\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    # remove empty strings\n",
    "    texts = [x for x in texts if x != '']\n",
    "    \n",
    "    return(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Normalize Corpus\n",
    "We will use [Amazon Food Review Corpus](https://www.kaggle.com/snap/amazon-fine-food-reviews). If we have already normalized and saved corpus with pickle, load the data. If not, load raw data and normalize the corpus and save it. This will takes some time since these codes are not optimized for preprocessing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"data/amazon_food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found amazon_corpus.pkl. Loading Corpus..\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(DATA_DIR,\"amazon_corpus.pkl\")):\n",
    "    print(\"Found amazon_corpus.pkl. Loading Corpus..\")\n",
    "    with open(os.path.join(DATA_DIR,\"amazon_corpus.pkl\"), \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print(\"No amazon_corpus.pkl. Preprocessing Corpus..\")\n",
    "    with open(os.path.join(DATA_DIR, \"Reviews.csv\"), 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)\n",
    "        corpus = [line[-1] for line in reader]\n",
    "\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    corpus = normalize_corpus(corpus, stops)\n",
    "\n",
    "    corpus = [line.split(' ') for line in corpus]\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, \"amazon_corpus.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387867"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten(corpus[:10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe.GloVeModel(150, 5, max_vocab_size=100000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.fit_to_corpus(corpus[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TensorBoard summaries to log/glove\n",
      "Saving TensorFlow models to save/glove\n",
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 7115724\n",
      "Total number of batches: 3495\n",
      "--------------------------------------------------------------------------------\n",
      "step: 1000, epoch:1, time/batch: 0.0035736639499664305, avg_loss: 164.47706604003906\n",
      "step: 2000, epoch:1, time/batch: 0.0035365924835205078, avg_loss: 76.98371887207031\n",
      "step: 3000, epoch:1, time/batch: 0.003534843683242798, avg_loss: 49.760887145996094\n",
      "step: 4000, epoch:2, time/batch: 0.003539583683013916, avg_loss: 115.78126525878906\n",
      "step: 5000, epoch:2, time/batch: 0.0035061142444610597, avg_loss: 100.78335571289062\n",
      "Saved summaries at step 5000\n",
      "Saved a model at step 5000\n",
      "step: 6000, epoch:2, time/batch: 0.0034685146808624266, avg_loss: 84.24215698242188\n",
      "step: 7000, epoch:3, time/batch: 0.0033266327381134032, avg_loss: 117.70085906982422\n",
      "step: 8000, epoch:3, time/batch: 0.003350919485092163, avg_loss: 95.9284439086914\n",
      "step: 9000, epoch:3, time/batch: 0.0035200517177581786, avg_loss: 81.59432983398438\n",
      "step: 10000, epoch:3, time/batch: 0.0035436339378356932, avg_loss: 72.432861328125\n",
      "Saved summaries at step 10000\n",
      "Saved a model at step 10000\n",
      "step: 11000, epoch:4, time/batch: 0.003402160406112671, avg_loss: 71.8412094116211\n",
      "step: 12000, epoch:4, time/batch: 0.003420959234237671, avg_loss: 60.996803283691406\n",
      "step: 13000, epoch:4, time/batch: 0.003395991325378418, avg_loss: 56.11250686645508\n",
      "step: 14000, epoch:5, time/batch: 0.003501730442047119, avg_loss: 62.264705657958984\n",
      "step: 15000, epoch:5, time/batch: 0.0035150470733642577, avg_loss: 56.85761260986328\n",
      "Saved summaries at step 15000\n",
      "Saved a model at step 15000\n",
      "step: 16000, epoch:5, time/batch: 0.003429037809371948, avg_loss: 48.54035568237305\n",
      "step: 17000, epoch:5, time/batch: 0.0033792831897735596, avg_loss: 42.27998733520508\n",
      "step: 18000, epoch:6, time/batch: 0.0033746440410614012, avg_loss: 55.9849739074707\n",
      "step: 19000, epoch:6, time/batch: 0.00355194616317749, avg_loss: 46.43285369873047\n",
      "step: 20000, epoch:6, time/batch: 0.003531832695007324, avg_loss: 43.45237731933594\n",
      "Saved summaries at step 20000\n",
      "Saved a model at step 20000\n",
      "step: 21000, epoch:7, time/batch: 0.0035029020309448244, avg_loss: 54.63960647583008\n",
      "step: 22000, epoch:7, time/batch: 0.0033341448307037352, avg_loss: 46.678218841552734\n",
      "step: 23000, epoch:7, time/batch: 0.0034830336570739746, avg_loss: 38.71757888793945\n",
      "step: 24000, epoch:7, time/batch: 0.0035327463150024415, avg_loss: 37.52936553955078\n",
      "step: 25000, epoch:8, time/batch: 0.00355063271522522, avg_loss: 42.28274917602539\n",
      "Saved summaries at step 25000\n",
      "Saved a model at step 25000\n",
      "step: 26000, epoch:8, time/batch: 0.003383427381515503, avg_loss: 37.62327194213867\n",
      "step: 27000, epoch:8, time/batch: 0.003425919771194458, avg_loss: 33.65017318725586\n",
      "step: 28000, epoch:9, time/batch: 0.0035485107898712157, avg_loss: 46.9813346862793\n",
      "step: 29000, epoch:9, time/batch: 0.003655414342880249, avg_loss: 40.619667053222656\n",
      "step: 30000, epoch:9, time/batch: 0.003591501235961914, avg_loss: 33.857906341552734\n",
      "Saved summaries at step 30000\n",
      "Saved a model at step 30000\n",
      "step: 31000, epoch:9, time/batch: 0.003414905548095703, avg_loss: 31.09050941467285\n",
      "step: 32000, epoch:10, time/batch: 0.003376342296600342, avg_loss: 38.59661865234375\n",
      "step: 33000, epoch:10, time/batch: 0.0035167665481567385, avg_loss: 32.80926513671875\n",
      "step: 34000, epoch:10, time/batch: 0.003591294288635254, avg_loss: 29.0560359954834\n"
     ]
    }
   ],
   "source": [
    "glove.train(10, log_dir=\"log/02_glove\", save_dir=\"save/02_glove\", print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, model):\n",
    "    target_V = model.embedding_for(target)\n",
    "    \n",
    "    similarities = []\n",
    "    for word in model.words:\n",
    "        if word == target: continue\n",
    "        \n",
    "        vector = model.embedding_for(word)\n",
    "        cosine_sim = 1 - spatial.distance.cosine(target_V, vector)\n",
    "        similarities.append([word, cosine_sim])\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['product', 0.6368147730827332],\n",
       " ['good', 0.5886852145195007],\n",
       " ['also', 0.5703610777854919],\n",
       " ['like', 0.5643731951713562],\n",
       " ['buy', 0.5581755042076111],\n",
       " ['really', 0.5555024147033691],\n",
       " ['much', 0.5272473692893982],\n",
       " ['well', 0.5267638564109802],\n",
       " ['love', 0.5197827219963074],\n",
       " ['best', 0.5129992365837097]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity('great', glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.generate_tsne('log/02_glove/fig.png', size=(15,15), word_count=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
