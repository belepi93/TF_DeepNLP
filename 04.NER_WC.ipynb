{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Named Entity Recognition with Window Classifier\n",
    "\n",
    "We will perform NER(Named Entity Recognition) with Window Classifier. As you may have already noticed, non-feedforward neural networks like RNN, GRU, LSTM will work well in these kinds of tasks. So we will revisit NER after we will have covered those networks.\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 4](http://web.stanford.edu/class/cs224n/lectures/lecture4.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WindowClassifier\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.conll2002.iob_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for sent in corpus:\n",
    "    words, _, tags = list(zip(*sent))\n",
    "    data.append([words, tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35651\n",
      "[('Sao', 'Paulo', '(', 'Brasil', ')', ',', '23', 'may', '(', 'EFECOM', ')', '.'), ('B-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/dev/test data\n",
    "random.seed(1004)\n",
    "random.shuffle(data)\n",
    "idx1 = int(len(data) * 0.6)\n",
    "idx2 = int(len(data) * 0.8)\n",
    "train_data = data[:idx1]\n",
    "valid_data = data[idx1:idx2]\n",
    "test_data = data[idx2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Train WindowClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04132118\n"
     ]
    }
   ],
   "source": [
    "model = WindowClassifier.WindowClassifier(word_embedding_size=100,\n",
    "                                          window_size=5,\n",
    "                                          hidden_size=300,\n",
    "                                          learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_data(train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 5280809\n",
      "--------------------------------------------------------------------------------\n",
      "001000: 1 [01000/03170], train_loss = 0.42526925, secs/batch = 0.0031\n",
      "002000: 1 [02000/03170], train_loss = 0.48944670, secs/batch = 0.0035\n",
      "003000: 1 [03000/03170], train_loss = 0.28266060, secs/batch = 0.0036\n",
      "Epoch training time: 10.522184371948242\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.68      0.54      0.60      2316\n",
      "     B-MISC       0.71      0.19      0.30      1680\n",
      "      B-ORG       0.70      0.55      0.62      2816\n",
      "      B-PER       0.71      0.48      0.57      2542\n",
      "      I-LOC       0.68      0.21      0.32       636\n",
      "     I-MISC       0.60      0.15      0.24      1260\n",
      "      I-ORG       0.66      0.49      0.57      2034\n",
      "      I-PER       0.79      0.57      0.66      1918\n",
      "\n",
      "avg / total       0.70      0.45      0.53     15202\n",
      "\n",
      "Finished Epoch 1\n",
      "train_loss = 0.37482945, validation_loss = 0.24070855\n",
      "\n",
      "004170: 2 [01000/03170], train_loss = 0.05110983, secs/batch = 0.0031\n",
      "005170: 2 [02000/03170], train_loss = 0.09348825, secs/batch = 0.0030\n",
      "006170: 2 [03000/03170], train_loss = 0.18916401, secs/batch = 0.0036\n",
      "Epoch training time: 10.113415241241455\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.74      0.66      0.70      2316\n",
      "     B-MISC       0.68      0.42      0.52      1680\n",
      "      B-ORG       0.78      0.64      0.70      2816\n",
      "      B-PER       0.81      0.61      0.70      2542\n",
      "      I-LOC       0.60      0.44      0.51       636\n",
      "     I-MISC       0.35      0.50      0.41      1260\n",
      "      I-ORG       0.82      0.47      0.60      2034\n",
      "      I-PER       0.84      0.73      0.78      1918\n",
      "\n",
      "avg / total       0.74      0.58      0.64     15202\n",
      "\n",
      "Finished Epoch 2\n",
      "train_loss = 0.18609206, validation_loss = 0.18507931\n",
      "\n",
      "007340: 3 [01000/03170], train_loss = 0.12501580, secs/batch = 0.0030\n",
      "008340: 3 [02000/03170], train_loss = 0.07968949, secs/batch = 0.0032\n",
      "009340: 3 [03000/03170], train_loss = 0.05422244, secs/batch = 0.0034\n",
      "Epoch training time: 10.133190870285034\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.77      0.72      0.74      2316\n",
      "     B-MISC       0.71      0.49      0.58      1680\n",
      "      B-ORG       0.78      0.73      0.75      2816\n",
      "      B-PER       0.88      0.67      0.76      2542\n",
      "      I-LOC       0.64      0.53      0.58       636\n",
      "     I-MISC       0.52      0.45      0.48      1260\n",
      "      I-ORG       0.75      0.68      0.72      2034\n",
      "      I-PER       0.85      0.76      0.80      1918\n",
      "\n",
      "avg / total       0.77      0.66      0.71     15202\n",
      "\n",
      "Finished Epoch 3\n",
      "train_loss = 0.11006011, validation_loss = 0.16004329\n",
      "\n",
      "010510: 4 [01000/03170], train_loss = 0.03759093, secs/batch = 0.0030\n",
      "011510: 4 [02000/03170], train_loss = 0.00727181, secs/batch = 0.0033\n",
      "012510: 4 [03000/03170], train_loss = 0.04961383, secs/batch = 0.0030\n",
      "Epoch training time: 10.057610750198364\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.83      0.69      0.76      2316\n",
      "     B-MISC       0.73      0.51      0.60      1680\n",
      "      B-ORG       0.72      0.81      0.76      2816\n",
      "      B-PER       0.86      0.74      0.80      2542\n",
      "      I-LOC       0.50      0.60      0.55       636\n",
      "     I-MISC       0.45      0.53      0.48      1260\n",
      "      I-ORG       0.69      0.73      0.71      2034\n",
      "      I-PER       0.90      0.76      0.82      1918\n",
      "\n",
      "avg / total       0.75      0.70      0.72     15202\n",
      "\n",
      "Finished Epoch 4\n",
      "train_loss = 0.06327088, validation_loss = 0.17120210\n",
      "\n",
      "013680: 5 [01000/03170], train_loss = 0.02856268, secs/batch = 0.0026\n",
      "014680: 5 [02000/03170], train_loss = 0.00065320, secs/batch = 0.0031\n",
      "015680: 5 [03000/03170], train_loss = 0.00232559, secs/batch = 0.0034\n",
      "Epoch training time: 10.169214487075806\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.80      0.76      0.78      2316\n",
      "     B-MISC       0.63      0.58      0.60      1680\n",
      "      B-ORG       0.81      0.77      0.79      2816\n",
      "      B-PER       0.88      0.75      0.81      2542\n",
      "      I-LOC       0.68      0.53      0.60       636\n",
      "     I-MISC       0.45      0.56      0.49      1260\n",
      "      I-ORG       0.74      0.70      0.72      2034\n",
      "      I-PER       0.86      0.79      0.82      1918\n",
      "\n",
      "avg / total       0.76      0.71      0.73     15202\n",
      "\n",
      "Finished Epoch 5\n",
      "train_loss = 0.03593379, validation_loss = 0.18761669\n",
      "\n",
      "016850: 6 [01000/03170], train_loss = 0.04471862, secs/batch = 0.0027\n",
      "017850: 6 [02000/03170], train_loss = 0.02966843, secs/batch = 0.0035\n",
      "018850: 6 [03000/03170], train_loss = 0.00105126, secs/batch = 0.0031\n",
      "Epoch training time: 10.131097555160522\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.77      0.75      0.76      2316\n",
      "     B-MISC       0.63      0.55      0.59      1680\n",
      "      B-ORG       0.74      0.80      0.77      2816\n",
      "      B-PER       0.84      0.76      0.80      2542\n",
      "      I-LOC       0.65      0.57      0.61       636\n",
      "     I-MISC       0.52      0.49      0.50      1260\n",
      "      I-ORG       0.73      0.72      0.72      2034\n",
      "      I-PER       0.90      0.76      0.82      1918\n",
      "\n",
      "avg / total       0.75      0.71      0.72     15202\n",
      "\n",
      "Finished Epoch 6\n",
      "train_loss = 0.01991071, validation_loss = 0.23112940\n",
      "\n",
      "020020: 7 [01000/03170], train_loss = 0.00235950, secs/batch = 0.0030\n",
      "021020: 7 [02000/03170], train_loss = 0.00674025, secs/batch = 0.0029\n",
      "022020: 7 [03000/03170], train_loss = 0.00352012, secs/batch = 0.0034\n",
      "Epoch training time: 10.13142728805542\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.77      0.76      0.77      2316\n",
      "     B-MISC       0.61      0.56      0.58      1680\n",
      "      B-ORG       0.77      0.79      0.78      2816\n",
      "      B-PER       0.87      0.75      0.80      2542\n",
      "      I-LOC       0.72      0.56      0.63       636\n",
      "     I-MISC       0.61      0.43      0.50      1260\n",
      "      I-ORG       0.75      0.69      0.72      2034\n",
      "      I-PER       0.88      0.76      0.82      1918\n",
      "\n",
      "avg / total       0.77      0.70      0.73     15202\n",
      "\n",
      "Finished Epoch 7\n",
      "train_loss = 0.01234745, validation_loss = 0.28510955\n",
      "\n",
      "023190: 8 [01000/03170], train_loss = 0.00011414, secs/batch = 0.0029\n",
      "024190: 8 [02000/03170], train_loss = 0.00158026, secs/batch = 0.0034\n",
      "025190: 8 [03000/03170], train_loss = 0.00118810, secs/batch = 0.0034\n",
      "Epoch training time: 10.063969850540161\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.73      0.80      0.76      2316\n",
      "     B-MISC       0.64      0.54      0.59      1680\n",
      "      B-ORG       0.79      0.76      0.78      2816\n",
      "      B-PER       0.82      0.77      0.79      2542\n",
      "      I-LOC       0.58      0.65      0.62       636\n",
      "     I-MISC       0.52      0.49      0.51      1260\n",
      "      I-ORG       0.81      0.64      0.71      2034\n",
      "      I-PER       0.88      0.78      0.83      1918\n",
      "\n",
      "avg / total       0.75      0.70      0.72     15202\n",
      "\n",
      "Finished Epoch 8\n",
      "train_loss = 0.00897425, validation_loss = 0.28787194\n",
      "\n",
      "026360: 9 [01000/03170], train_loss = 0.00009259, secs/batch = 0.0036\n",
      "027360: 9 [02000/03170], train_loss = 0.00070487, secs/batch = 0.0035\n",
      "028360: 9 [03000/03170], train_loss = 0.00001272, secs/batch = 0.0042\n",
      "Epoch training time: 10.175343751907349\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.77      0.76      0.77      2316\n",
      "     B-MISC       0.68      0.52      0.59      1680\n",
      "      B-ORG       0.79      0.79      0.79      2816\n",
      "      B-PER       0.87      0.75      0.81      2542\n",
      "      I-LOC       0.70      0.54      0.61       636\n",
      "     I-MISC       0.53      0.49      0.51      1260\n",
      "      I-ORG       0.74      0.69      0.71      2034\n",
      "      I-PER       0.89      0.76      0.82      1918\n",
      "\n",
      "avg / total       0.77      0.70      0.73     15202\n",
      "\n",
      "Finished Epoch 9\n",
      "train_loss = 0.00756210, validation_loss = 0.32674231\n",
      "\n",
      "029530: 10 [01000/03170], train_loss = 0.00000190, secs/batch = 0.0037\n",
      "030530: 10 [02000/03170], train_loss = 0.00024280, secs/batch = 0.0033\n",
      "031530: 10 [03000/03170], train_loss = 0.00010522, secs/batch = 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training time: 10.2492835521698\n",
      "\n",
      "Evaluating..\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.75      0.77      0.76      2316\n",
      "     B-MISC       0.57      0.58      0.58      1680\n",
      "      B-ORG       0.80      0.76      0.78      2816\n",
      "      B-PER       0.80      0.80      0.80      2542\n",
      "      I-LOC       0.66      0.58      0.62       636\n",
      "     I-MISC       0.58      0.49      0.53      1260\n",
      "      I-ORG       0.73      0.74      0.73      2034\n",
      "      I-PER       0.89      0.78      0.83      1918\n",
      "\n",
      "avg / total       0.74      0.72      0.73     15202\n",
      "\n",
      "Finished Epoch 10\n",
      "train_loss = 0.00696038, validation_loss = 0.29741616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(10, save_dir=\"save/04_ner\", log_dir=\"log/04_ner\", print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "According to [Named Entity Recognition with Character-Level Models - Klein et al.](https://nlp.stanford.edu/cmanning/papers/conll-ner.pdf), \"*because of data sparsity, sophisticated\n",
    "unknown word models are generally required for good performance.*\"\n",
    "\n",
    "But in this model, we will just ignore unknown words in test time. We will embed unknown words to zero-vector for convenience. Maybe we will go deeper into NER after we cover some CNN and RNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/04_ner/epoch010_0.2974.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 5280809\n",
      "--------------------------------------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.75      0.78      0.76      2237\n",
      "     B-MISC       0.56      0.59      0.58      1608\n",
      "      B-ORG       0.81      0.74      0.77      2963\n",
      "      B-PER       0.80      0.83      0.81      2534\n",
      "      I-LOC       0.66      0.57      0.61       615\n",
      "     I-MISC       0.56      0.42      0.48      1305\n",
      "      I-ORG       0.74      0.75      0.75      2043\n",
      "      I-PER       0.89      0.83      0.86      1859\n",
      "\n",
      "avg / total       0.74      0.72      0.73     15164\n",
      "\n",
      "test samples: 135642, time elapsed: 0.7597, time per one batch: 0.0007\n"
     ]
    }
   ],
   "source": [
    "model.test(test_data, load_dir=\"save/04_ner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
