{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Named Entity Recognition with Window Classifier\n",
    "\n",
    "We will perform NER(Named Entity Recognition) with Window Classifier. As you may have already noticed, non-feedforward neural networks like RNN, GRU, LSTM will work well in these kinds of tasks. So we will revisit NER after we will have covered those networks.\n",
    "\n",
    "### References\n",
    "- [CS224n: Natural Language Processing with Deep Learning - Lecture 4](http://web.stanford.edu/class/cs224n/lectures/lecture4.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WindowClassifier\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.conll2002.iob_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for sent in corpus:\n",
    "    words, _, tags = list(zip(*sent))\n",
    "    data.append([words, tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35651\n",
      "[('Sao', 'Paulo', '(', 'Brasil', ')', ',', '23', 'may', '(', 'EFECOM', ')', '.'), ('B-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test data\n",
    "random.seed(1004)\n",
    "random.shuffle(data)\n",
    "idx = int(len(data) * 0.8)\n",
    "train_data = data[:idx]\n",
    "test_data = data[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Train WindowClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: 04132118\n"
     ]
    }
   ],
   "source": [
    "model = WindowClassifier.WindowClassifier(word_embedding_size=100,\n",
    "                                          window_size=5,\n",
    "                                          hidden_size=300,\n",
    "                                          learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/young/.virtualenv/NLP/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model.fit_to_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TensorBoard summaries to log/04_ner\n",
      "Saving TensorFlow models to save/04_ner\n",
      "--------------------------------------------------------------------------------\n",
      "Created and Initialized fresh model. Size: 6130409\n",
      "Total number of batches: 4241\n",
      "--------------------------------------------------------------------------------\n",
      "step: 1000, epoch:1, time/batch: 0.004488, avg_loss: 0.5008\n",
      "step: 2000, epoch:1, time/batch: 0.004278, avg_loss: 0.3411\n",
      "step: 3000, epoch:1, time/batch: 0.004164, avg_loss: 0.2906\n",
      "step: 4000, epoch:1, time/batch: 0.0042, avg_loss: 0.2473\n",
      "step: 5000, epoch:2, time/batch: 0.00419, avg_loss: 0.1753\n",
      "Saved summaries at step 5000\n",
      "Saved a model at step 5000\n",
      "step: 6000, epoch:2, time/batch: 0.003725, avg_loss: 0.1726\n",
      "step: 7000, epoch:2, time/batch: 0.003706, avg_loss: 0.1559\n",
      "step: 8000, epoch:2, time/batch: 0.004471, avg_loss: 0.1451\n",
      "step: 9000, epoch:3, time/batch: 0.004477, avg_loss: 0.09224\n",
      "step: 10000, epoch:3, time/batch: 0.004463, avg_loss: 0.09264\n",
      "Saved summaries at step 10000\n",
      "Saved a model at step 10000\n",
      "step: 11000, epoch:3, time/batch: 0.003808, avg_loss: 0.09465\n",
      "step: 12000, epoch:3, time/batch: 0.003593, avg_loss: 0.09492\n",
      "step: 13000, epoch:4, time/batch: 0.004456, avg_loss: 0.04723\n",
      "step: 14000, epoch:4, time/batch: 0.005847, avg_loss: 0.05418\n",
      "step: 15000, epoch:4, time/batch: 0.007342, avg_loss: 0.05413\n",
      "Saved summaries at step 15000\n",
      "Saved a model at step 15000\n",
      "step: 16000, epoch:4, time/batch: 0.003881, avg_loss: 0.05733\n",
      "step: 17000, epoch:5, time/batch: 0.003588, avg_loss: 0.01652\n",
      "step: 18000, epoch:5, time/batch: 0.004099, avg_loss: 0.02691\n",
      "step: 19000, epoch:5, time/batch: 0.004472, avg_loss: 0.0321\n",
      "step: 20000, epoch:5, time/batch: 0.004465, avg_loss: 0.03008\n",
      "Saved summaries at step 20000\n",
      "Saved a model at step 20000\n",
      "step: 21000, epoch:5, time/batch: 0.003826, avg_loss: 0.03282\n",
      "step: 22000, epoch:6, time/batch: 0.003563, avg_loss: 0.01265\n",
      "step: 23000, epoch:6, time/batch: 0.004673, avg_loss: 0.01792\n",
      "step: 24000, epoch:6, time/batch: 0.005073, avg_loss: 0.01908\n",
      "step: 25000, epoch:6, time/batch: 0.005075, avg_loss: 0.0211\n",
      "Saved summaries at step 25000\n",
      "Saved a model at step 25000\n",
      "step: 26000, epoch:7, time/batch: 0.003657, avg_loss: 0.00955\n",
      "step: 27000, epoch:7, time/batch: 0.003469, avg_loss: 0.01044\n",
      "step: 28000, epoch:7, time/batch: 0.003479, avg_loss: 0.01184\n",
      "step: 29000, epoch:7, time/batch: 0.003949, avg_loss: 0.01473\n",
      "step: 30000, epoch:8, time/batch: 0.004259, avg_loss: 0.008374\n",
      "Saved summaries at step 30000\n",
      "Saved a model at step 30000\n",
      "step: 31000, epoch:8, time/batch: 0.003856, avg_loss: 0.007059\n",
      "step: 32000, epoch:8, time/batch: 0.003474, avg_loss: 0.008533\n",
      "step: 33000, epoch:8, time/batch: 0.0035, avg_loss: 0.009291\n",
      "step: 34000, epoch:9, time/batch: 0.003547, avg_loss: 0.004034\n",
      "step: 35000, epoch:9, time/batch: 0.004788, avg_loss: 0.005562\n",
      "Saved summaries at step 35000\n",
      "Saved a model at step 35000\n",
      "step: 36000, epoch:9, time/batch: 0.003698, avg_loss: 0.007324\n",
      "step: 37000, epoch:9, time/batch: 0.003485, avg_loss: 0.0105\n",
      "step: 38000, epoch:9, time/batch: 0.003497, avg_loss: 0.006878\n",
      "step: 39000, epoch:10, time/batch: 0.003756, avg_loss: 0.00392\n",
      "step: 40000, epoch:10, time/batch: 0.004147, avg_loss: 0.004659\n",
      "Saved summaries at step 40000\n",
      "Saved a model at step 40000\n",
      "step: 41000, epoch:10, time/batch: 0.003662, avg_loss: 0.006748\n",
      "step: 42000, epoch:10, time/batch: 0.003463, avg_loss: 0.01015\n"
     ]
    }
   ],
   "source": [
    "model.train(10, save_dir=\"save/04_ner\", log_dir=\"log/04_ner\", print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "According to [Named Entity Recognition with Character-Level Models - Klein et al.](https://nlp.stanford.edu/cmanning/papers/conll-ner.pdf), \"*because of data sparsity, sophisticated\n",
    "unknown word models are generally required for good performance.*\"\n",
    "\n",
    "But in this model, we will just ignore unknown words in test time. We will embed unknown words to zero-vector for convenience. Maybe we will go deeper into NER after we cover some CNN and RNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/04_ner/WC_NER-42401.model\n",
      "--------------------------------------------------------------------------------\n",
      "Restored model from checkpoint for testing. Size: 6130409\n",
      "--------------------------------------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC       0.82      0.78      0.79      2237\n",
      "     B-MISC       0.68      0.63      0.65      1608\n",
      "      B-ORG       0.82      0.80      0.81      2963\n",
      "      B-PER       0.90      0.84      0.87      2534\n",
      "      I-LOC       0.68      0.58      0.63       615\n",
      "     I-MISC       0.57      0.52      0.54      1305\n",
      "      I-ORG       0.78      0.77      0.77      2043\n",
      "      I-PER       0.90      0.84      0.87      1859\n",
      "\n",
      "avg / total       0.79      0.75      0.77     15164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.test(test_data, load_dir=\"save/04_ner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
